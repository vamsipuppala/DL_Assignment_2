{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W6owH3k8uOMq"
   },
   "source": [
    "# Problem 1 - Variational Auto-Encoder (VAE)\n",
    "\n",
    "Variational Auto-Encoders (VAEs) are a widely used class of generative models. They are simple to implement and, in contrast to other generative model classes like Generative Adversarial Networks (GANs, see Problem 2), they optimize an explicit maximum likelihood objective to train the model. Finally, their architecture makes them well-suited for unsupervised representation learning, i.e., learning low-dimensional representations of high-dimenionsal inputs, like images, with only self-supervised objectives (data reconstruction in the case of VAEs).\n",
    "\n",
    "![VAE_sketch.png](data:image/png;base64,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)\n",
    "*(image source: https://mlexplained.com/2017/12/28/an-intuitive-explanation-of-variational-autoencoders-vaes-part-1)*\n",
    "\n",
    "**By working on this problem you will learn and practice the following steps:**\n",
    "1. Set up a data loading pipeline in PyTorch.\n",
    "2. Implement, train and visualize an auto-encoder architecture.\n",
    "3. Extend your implementation to a variational auto-encoder.\n",
    "4. Learn how to tune the critical beta parameter of your VAE.\n",
    "5. Inspect the learned representation of your VAE.\n",
    "6. Extend VAE's generative capabilities by conditioning it on the label you wish to generate.\n",
    "\n",
    "\n",
    "**Note**: For faster training of the models in this assignment you can enable GPU support in this Colab. Navigate to \"Runtime\" --> \"Change Runtime Type\" and set the \"Hardware Accelerator\" to \"GPU\". However, you might hit compute limits of the colab free edition. Hence, you might want to debug locally (e.g. in a jupyter notebook) or in a CPU-only runtime on colab."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "53lY_6Giu_7o"
   },
   "source": [
    "# 1. MNIST Dataset\n",
    "\n",
    "We will perform all experiments for this problem using the [MNIST dataset](http://yann.lecun.com/exdb/mnist/), a standard dataset of handwritten digits. The main benefits of this dataset are that it is small and relatively easy to model. It therefore allows for quick experimentation and serves as initial test bed in many papers.\n",
    "\n",
    "Another benefit is that it is so widely used that PyTorch even provides functionality to automatically download it.\n",
    "\n",
    "Let's start by downloading the data and visualizing some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "QEq5AXeVlQSC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for auto-reloading external modules\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 476,
     "referenced_widgets": [
      "25c51dbd460048a0ade87d231b919eba",
      "2cb89c5dca2444aa899dc73dc3e281f5",
      "e117f794f56b4c45894647d748a0f6f4",
      "61ede69ade0243fb8b9fbcdf8ccabf61",
      "ec99aa403334405184322466cdef6ded",
      "074608e2e6c646d8be29bee287d4eecd",
      "e41aa50f2c594af084fa1af6b9d9a4b3",
      "dd354536c91840b8af906237e2ad1540",
      "ebf2a93e8e29426c9a944ff9f6e53a2a",
      "d6abdefd833a4850a6d4fc5532a0a31f",
      "ec1a9588948e46bb85f9a2cfc2afe690",
      "4c207577fbf14d098de29220c1a200d9",
      "d09b2b8b271b46788d00e6a1442699de",
      "f321c1db62ad4645916b08460fd6b40a",
      "08d643e9e1ed4f5f8a4b938513eb74b2",
      "1069f776c52b4b53be617bbf368fcc5e",
      "5795e5119e744f3b95af1061b694019d",
      "e777a7f4889840be88dffb206d851bf4",
      "430e2502449c47a5a23f47eaf088a894",
      "316e858872234de9a3300a355a32df5d",
      "f8f09be5e62e446cb81ea9ba618a2b05",
      "cc44f8e2f9d94dac82132168dfa615cb",
      "5d868cebd70a4bcf9377ffb62f9adb86",
      "16c9106fb10d43c28815c73e0a960c32",
      "d885bf767bc64103a16be48fce9cb9a9",
      "7aab706d77ee449881a0a1e199a8e845",
      "251a7d7cccc243a7a9afb379f40b6abc",
      "5dbc78cf70d64fc094c758bc5c06b8bb",
      "d80c21ce156748ddae0ff0bcccaccf41",
      "a11343c41bcd4a71912a80830d5bd5b6",
      "f5ee2f461d0c41e8b03f2450ccb0845b",
      "00ccb5258094474484c970e4b698d0e9",
      "669a51ec535b4841b19bafc1cf734534",
      "f39a85e3cf3f4fdfb8141c16c90f5cee",
      "d4caee53c3e049c7ba72e00ba03d358a",
      "d1396522b53047149210c329510307fa",
      "53b496dc0326451994983d34e32eabcf",
      "dece586b88a74ffbb5dba17157cac40c",
      "55b2698f8bab4a2e9b40bad1ac28580c",
      "eee3ae88d3db4a05b23b7f6e14e176cb",
      "0c837a309a8749e9acb998d26b53eb03",
      "79a7c29698064a238d27747ed85b3442",
      "c84c00ce102843c1a5567338fa433747",
      "c1e61faf117747e49ba18e9e8c07c359"
     ]
    },
    "id": "Rdkn5v6WvNAa",
    "outputId": "e01140a7-6686-4c94-ed21-0117c997f89f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "\n",
      " Download complete! Downloaded 60000 training examples!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')   # use GPU if available\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# this will automatically download the MNIST training set\n",
    "mnist_train = torchvision.datasets.MNIST(root='./data', \n",
    "                                         train=True, \n",
    "                                         download=True, \n",
    "                                         transform=torchvision.transforms.ToTensor())\n",
    "print(\"\\n Download complete! Downloaded {} training examples!\".format(len(mnist_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "gVr1sOuWwk_F"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1000x5000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from numpy.random.mtrand import sample\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Let's display some of the training samples.\n",
    "sample_images = []\n",
    "randomize = False # set to False for debugging\n",
    "num_samples = 5 # simple data sampling for now, later we will use proper DataLoader\n",
    "if randomize:\n",
    "  sample_idxs = np.random.randint(low=0,high=len(mnist_train), size=num_samples)\n",
    "else:\n",
    "  sample_idxs = list(range(num_samples))\n",
    "\n",
    "for idx in sample_idxs:\n",
    "  sample = mnist_train[idx]\n",
    "  # print(f\"Tensor w/ shape {sample[0][0].detach().cpu().numpy().shape} and label {sample[1]}\")\n",
    "  sample_images.append(sample[0][0].data.cpu().numpy())\n",
    "  # print(sample_images[0]) # Values are in [0, 1]\n",
    "\n",
    "fig = plt.figure(figsize = (10, 50))   \n",
    "ax1 = plt.subplot(111)\n",
    "ax1.imshow(np.concatenate(sample_images, axis=1), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "suPd0gyiuvvo"
   },
   "source": [
    "# 2. Auto-Encoder\n",
    "\n",
    "Before implementing the full VAE, we will first implement an **auto-encoder architecture**. Auto-encoders feature the same encoder-decoder architecture as VAEs and therefore also learn a low-dimensional representation of the input data without supervision. In contrast to VAEs they are **fully deterministic** models and do not employ variational inference for optimization.\n",
    "\n",
    "The **architecture** is very simple: we will encode the input image into a low-dimensional representation using fully connected layers for the encoder. This results in a low-dimensional representation of the input image. This representation will get decoded back into the dimensionality of the input image using a decoder network that mirrors the architecture of the encoder. The whole model is trained by **minimizing a reconstruction loss** between the input and the decoded image.\n",
    "\n",
    "Intuitively, the **auto-encoder needs to compress the information contained in the input image** into a much lower dimensional representation (e.g. 28x28=784px vs. nz embedding dimensions for our MNIST model). This is possible since the information captured in the pixels is *highly redundant*. E.g. encoding an MNIST image requires <4 bits to encode which of the 10 possible digits is displayed and a few additional bits to capture information about shape and orientation. This is much less than the $255^{28\\cdot 28}$ bits of information that could be theoretically captured in the input image.\n",
    "\n",
    "Learning such a **compressed representation can make downstream task learning easier**. For example, learning to add two numbers based on the inferred digits is much easier than performing the task based on two piles of pixel values that depict the digits.\n",
    "\n",
    "In the following, we will first define the architecture of encoder and decoder and then train the auto-encoder model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akSPKMJ0l3rD"
   },
   "source": [
    "## Defining the Auto-Encoder Architecture [6pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "rteiFTqfuvTu"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "# Prob1-1: Let's define encoder and decoder networks\n",
    "class Encoder(nn.Module):\n",
    "  def __init__(self, nz, input_size):\n",
    "    super().__init__()\n",
    "    self.input_size = input_size\n",
    "    ################################# TODO #########################################\n",
    "    # Create the network architecture using a nn.Sequential module wrapper.        #\n",
    "    # Encoder Architecture:                                                        #\n",
    "    # - input_size -> 256                                                          #\n",
    "    # - ReLU                                                                       #\n",
    "    # - 256 -> 64                                                                  #\n",
    "    # - ReLU                                                                       #\n",
    "    # - 64 -> nz                                                                   #\n",
    "    # HINT: Verify the shapes of intermediate layers by running partial networks   #\n",
    "    #        (with the next notebook cell) and visualizing the output shapes.      #\n",
    "    ################################################################################\n",
    "    self.net = nn.Sequential(\n",
    "            nn.Linear(input_size, 256), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, nz),\n",
    "           \n",
    "        )\n",
    "    ################################ END TODO #######################################\n",
    "  \n",
    "  def forward(self, x):\n",
    "    return self.net(x)\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "  def __init__(self, nz, output_size):\n",
    "    super().__init__()\n",
    "    self.output_size = output_size\n",
    "    ################################# TODO #########################################\n",
    "    # Create the network architecture using a nn.Sequential module wrapper.        #\n",
    "    # Decoder Architecture (mirrors encoder architecture):                         #\n",
    "    # - nz -> 64                                                                   #\n",
    "    # - ReLU                                                                       #\n",
    "    # - 64 -> 256                                                                  #\n",
    "    # - ReLU                                                                       #\n",
    "    # - 256 -> output_size                                                         #\n",
    "    ################################################################################\n",
    "    self.net =  nn.Sequential(\n",
    "            nn.Linear(nz, 64), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, output_size),\n",
    "           \n",
    "        )\n",
    "    ################################ END TODO #######################################\n",
    "  \n",
    "  def forward(self, z):\n",
    "    return self.net(z).reshape(-1, 1, self.output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0PKgbP4l-n8"
   },
   "source": [
    "## Testing the Auto-Encoder Forward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "-m9ur743t-22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample_img.shape=torch.Size([64, 1, 28, 28]), <class 'torch.Tensor'>, input.shape=torch.Size([64, 784])\n",
      "Shape of encoding vector (should be [batch_size, nz]): torch.Size([64, 32])\n",
      "Shape of decoded image (should be [batch_size, 1, out_size]): torch.Size([64, 1, 784]).\n"
     ]
    }
   ],
   "source": [
    "# To test your encoder/decoder, let's encode/decode some sample images\n",
    "# first, make a PyTorch DataLoader object to sample data batches\n",
    "batch_size = 64\n",
    "nworkers = 2        # number of workers used for efficient data loading\n",
    "\n",
    "####################################################################################\n",
    "# Create a PyTorch DataLoader object for efficiently generating training batches.  #\n",
    "# Make sure that the data loader automatically shuffles the training dataset.      #\n",
    "# Consider only *full* batches of data, to avoid torch errrors.              #\n",
    "# The DataLoader wraps the MNIST dataset class we created earlier.           #\n",
    "#       Use the given batch_size and number of data loading workers when creating  #\n",
    "#       the DataLoader. https://pytorch.org/docs/stable/data.html                  #\n",
    "####################################################################################\n",
    "mnist_data_loader = torch.utils.data.DataLoader(mnist_train, \n",
    "                                                batch_size=batch_size, \n",
    "                                                shuffle=True, \n",
    "                                                num_workers=nworkers,\n",
    "                                                drop_last=True)\n",
    "####################################################################################\n",
    "\n",
    "# now we can run a forward pass for encoder and decoder and check the produced shapes\n",
    "in_size = out_size = 28*28 # image size\n",
    "nz = 32          # dimensionality of the learned embedding\n",
    "encoder = Encoder(nz=nz, input_size=in_size)\n",
    "decoder = Decoder(nz=nz, output_size=out_size)\n",
    "for sample_img, sample_label in mnist_data_loader: # loads a batch of data\n",
    "  input = sample_img.reshape([batch_size, in_size])\n",
    "  print(f'{sample_img.shape=}, {type(sample_img)}, {input.shape=}')\n",
    "  enc = encoder(input)\n",
    "  print(f\"Shape of encoding vector (should be [batch_size, nz]): {enc.shape}\")\n",
    "  dec = decoder(enc)\n",
    "  print(\"Shape of decoded image (should be [batch_size, 1, out_size]): {}.\".format(dec.shape))    \n",
    "  break\n",
    "\n",
    "del input, enc, dec, encoder, decoder, nworkers # remove to avoid confusion later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ssk-NnNE9vO"
   },
   "source": [
    "Now that we defined encoder and decoder network our architecture is nearly complete. However, before we start training, we can wrap encoder and decoder into an auto-encoder class for easier handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "LziabE_5E9IG"
   },
   "outputs": [],
   "source": [
    "class AutoEncoder(nn.Module):\n",
    "  def __init__(self, nz):\n",
    "    super().__init__()\n",
    "    self.encoder = Encoder(nz=nz, input_size=in_size)\n",
    "    self.decoder = Decoder(nz=nz, output_size=out_size)\n",
    "\n",
    "  def forward(self, x):\n",
    "    enc = self.encoder(x)\n",
    "    return self.decoder(enc)\n",
    "\n",
    "  def reconstruct(self, x):\n",
    "    \"\"\"Only used later for visualization.\"\"\"\n",
    "    enc = self.encoder(x)\n",
    "    flattened = self.decoder(enc)\n",
    "    image = flattened.reshape(-1, 28, 28)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wo2rQhAVEYp3"
   },
   "source": [
    "## Setting up the Auto-Encoder Training Loop [6pt]\n",
    "After implementing the network architecture, we can now set up the training loop and run training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "7vWaTyoqEnWK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device available cpu\n",
      "Run Epoch 0\n",
      "It 0: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 100: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 200: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 300: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 400: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 500: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 600: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 700: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 800: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 900: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Run Epoch 1\n",
      "It 1000: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 1100: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 1200: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 1300: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 1400: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 1500: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 1600: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 1700: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 1800: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Run Epoch 2\n",
      "It 1900: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 2000: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 2100: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 2200: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 2300: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 2400: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 2500: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 2600: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 2700: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 2800: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Run Epoch 3\n",
      "It 2900: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 3000: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 3100: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 3200: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 3300: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 3400: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 3500: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 3600: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 3700: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Run Epoch 4\n",
      "It 3800: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 3900: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 4000: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 4100: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 4200: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 4300: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 4400: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 4500: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 4600: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Run Epoch 5\n",
      "It 4700: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 4800: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 4900: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 5000: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 5100: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 5200: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 5300: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 5400: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 5500: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 5600: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Run Epoch 6\n",
      "It 5700: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 5800: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 5900: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 6000: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 6100: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 6200: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 6300: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 6400: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 6500: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Run Epoch 7\n",
      "It 6600: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 6700: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 6800: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 6900: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 7000: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 7100: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 7200: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 7300: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 7400: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Run Epoch 8\n",
      "It 7500: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 7600: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 7700: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 7800: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 7900: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 8000: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 8100: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 8200: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 8300: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It 8400: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Run Epoch 9\n",
      "It 8500: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 8600: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 8700: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 8800: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 8900: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 9000: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 9100: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 9200: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "It 9300: Reconstruction Loss: tensor([[[ 1.3820e-03,  3.5825e-04, -2.1482e-04,  ...,  3.1660e-04,\n",
      "          -7.5383e-04,  3.0298e-04]],\n",
      "\n",
      "        [[ 1.3543e-03, -8.7826e-05, -7.1146e-04,  ...,  7.9298e-04,\n",
      "          -7.1036e-04, -1.3400e-04]],\n",
      "\n",
      "        [[ 1.2056e-03, -2.4128e-04,  9.8401e-05,  ...,  2.5295e-04,\n",
      "          -1.3691e-03,  5.9947e-04]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 1.5905e-03, -5.4829e-04, -8.1386e-04,  ...,  8.0120e-04,\n",
      "          -4.8573e-04,  1.1768e-03]],\n",
      "\n",
      "        [[ 9.5962e-04,  2.9410e-05, -7.3437e-05,  ..., -4.8086e-04,\n",
      "          -4.1472e-04,  4.3033e-04]],\n",
      "\n",
      "        [[ 1.7237e-03, -4.4358e-04, -7.3850e-04,  ...,  6.6811e-04,\n",
      "          -1.2990e-03,  5.1124e-04]]], grad_fn=<ReshapeAliasBackward0>)\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# Prob1-2\n",
    "epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# build AE model\n",
    "print(f'Device available {device}')\n",
    "ae_model = AutoEncoder(nz).to(device)    # transfer model to GPU if available\n",
    "ae_model = ae_model.train()   # set model in train mode (eg batchnorm params get updated)\n",
    "\n",
    "# build optimizer and loss function\n",
    "####################################### TODO #######################################\n",
    "# Build the optimizer and loss classes. For the loss you can use a loss layer      #\n",
    "# from the torch.nn package. We recommend binary cross entropy.                    #\n",
    "# HINT: We will use the Adam optimizer (learning rate given above, otherwise       #\n",
    "#       default parameters).                                                       #\n",
    "# NOTE: We could also use alternative losses like MSE and cross entropy, depending #\n",
    "#       on the assumptions we are making about the output distribution.            #\n",
    "####################################################################################\n",
    "# loss_fun = nn.BCELoss()\n",
    "loss_fun = nn.MSELoss()\n",
    "\n",
    "opt = torch.optim.Adam(ae_model.parameters(), lr = learning_rate)\n",
    "#################################### END TODO #######################################\n",
    "\n",
    "train_it = 0\n",
    "for ep in range(epochs):\n",
    "  print(\"Run Epoch {}\".format(ep))\n",
    "  ####################################### TODO #######################################\n",
    "  # Implement the main training loop for the auto-encoder model.                     #\n",
    "  # HINT: Your training loop should sample batches from the data loader, run the     #\n",
    "  #       forward pass of the AE, compute the loss, perform the backward pass and    #\n",
    "  #       perform one gradient step with the optimizer.                              #\n",
    "  # HINT: Don't forget to erase old gradients before performing the backward pass.   #\n",
    "  ####################################################################################\n",
    "  for sample_img, sample_label in mnist_data_loader:\n",
    "    input_img = sample_img.reshape([batch_size, in_size])\n",
    "    \n",
    "    out_values = ae_model(input_img)\n",
    "#     print(input_img.shape, rec_loss.shape, sample_img.shape, input_img.reshape([batch_size,1, in_size]).shape)\n",
    "    loss = loss_fun(out_values, input_img.reshape([batch_size,1, in_size]))\n",
    "\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "\n",
    "    if train_it % 100 == 0:\n",
    "      print(\"It {}: Reconstruction Loss: {}\".format(train_it, rec_loss))\n",
    "    train_it += 1\n",
    "  #################################### END TODO #####################################\n",
    "\n",
    "print(\"Done!\")\n",
    "del epochs, learning_rate, sample_img, train_it, rec_loss #, opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3blvOcuUnKpp"
   },
   "source": [
    "## Verifying reconstructions\n",
    "Now that we trained the auto-encoder we can visualize some of the reconstructions on the test set to verify that it is converged and did not overfit. **Before continuing, make sure that your auto-encoder is able to reconstruct these samples near-perfectly.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "HmbdWzLxHuXV"
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABj0AAAKXCAYAAADZz4gLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABjo0lEQVR4nO3deZBe1Xkn/vP23toaLUiN0IIEAoTFYhZjCA5KbEiwjSdFTZyExPZMZqbiYDsmxGPHYWaiSSVSQioMkzAmIZXyeCZh8EyN15RjowQHj03ZIBmZTQgwQghJjRYkdWvr9f394UI/y2zPI3TVrcvnU9VVVvfXV8977znnnvs+vOpGs9lsFgAAAAAAgBNcy3gXAAAAAAAAcCxoegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUQtt4F/CTxsbGytatW8vUqVNLo9EY73IAAAAAAIBx1Gw2y8DAQJk7d25paXntz3JMuKbH1q1by/z588e7DAAAAAAAYALZvHlzmTdv3mtmJlzTY+rUqaWUUs4888zS2to6ztUAAAAAAADjaXR0tDz55JOH+wevpbKmx2c+85nyp3/6p2Xbtm3lLW95S7ntttvKO97xjtf9/730T1q1trZqegAAAAAAAKWUEvqVGJX8IvPPf/7z5cYbbyw333xzeeihh8o73vGOcs0115Tnnnuuir8OAAAAAACgNJrNZvNYH/TSSy8tF154YbnjjjsOf2/p0qXlF37hF8qqVate8//b399fenp6ytKlS33SAwAAAAAA3uRGR0fL+vXry969e8u0adNeM3vMP+kxNDRU1q5dW66++uojvn/11VeX+++//2X5wcHB0t/ff8QXAAAAAABA1jFveuzcubOMjo6WOXPmHPH9OXPmlL6+vpflV61aVXp6eg5/zZ8//1iXBAAAAAAAvAlU8js9Snn5LxRpNpuv+EtGPv3pT5e9e/ce/tq8eXNVJQEAAAAAADXWdqwPOGvWrNLa2vqyT3Vs3779ZZ/+KKWUzs7O0tnZeazLAAAAAAAA3mSO+Sc9Ojo6ykUXXVRWr159xPdXr15dLr/88mP91wEAAAAAAJRSKvikRyml3HTTTeUDH/hAufjii8tll11W7rzzzvLcc8+VD3/4w1X8dQAAAAAAANU0PX7pl36p7Nq1q/zBH/xB2bZtW1m2bFn52te+VhYuXFjFXwcAAAAAAFAazWazOd5F/Lj+/v7S09NTli5dWlpbW8e7HAAAAAAAYByNjo6W9evXl71795Zp06a9ZvaY/04PAAAAAACA8aDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALbeNdAABMZNdcc00429YWv60uWLAgnL3kkkvC2Yx77703nH366afD2fvvv/9oygEAAAB4w3zSAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqQdMDAAAAAACoBU0PAAAAAACgFtrGuwAAON5uuOGGcPaSSy6psJKYZrNZyXF/5md+Jpx9y1veEs5u2LAhnN21a1c4C3As9fb2hrOrVq0KZ//u7/4unP3Hf/zHcBY4/jo7O8PZX/qlXwpnM3uwZ599Npy9/fbbw1l7MADqzCc9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqoW28CwCAY+GGG24IZy+55JIKK4nZtm1bOPvII4+EsyeffHI4+9a3vjWcnT17djh72WWXhbN///d/H84CHEvz588PZ5vNZji7e/fuoykHmIBOOumkcPbKK68MZ8fGxsLZ0047LZy94IILwtl/+qd/CmeB42/hwoXh7Mc+9rFw9hOf+MTRlPOmtmzZsnB269at4eyLL754NOUQ5JMeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUQtt4FwAAr+S0005L5S+66KJK6tiyZUs4e9ttt4WzAwMD4ezg4GA429raGs7+p//0n8LZBQsWhLOTJ08OZwHGS+Y+k1mH165dexTVAMfLlClTwtl/+2//bYWVALy6c889N5xtb2+vsBLe+ta3hrPveMc7wtk77rjjaMohyCc9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqoW28C+DVXXzxxeHs8uXLU8fes2dPODs8PBzO3n///ZXUsGPHjnAWqIeTTjqpsmNv2bIlnP3TP/3TcHbv3r1HU84xdc0114Szc+fOraSGhx9+uJLjAryeefPmhbPvete7wtnvfOc7R1MOcJxcddVV4eyFF14Yzi5evPhoyhk3Z511VjjbaDTC2c2bN4ezGzZsCGfhzaalJf7fnp933nkVVkLGxo0bw9mf//mfD2c7OztTdQwODqbyb3Y+6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAtt410Ar+6XfumXwtlZs2ZVWEnc8uXLw9lDhw6Fs1u2bDmKaoh68cUXw9mvfe1rqWM/++yzyWrgR9atW5fKf/KTnwxnM+vP/v37U3WMt0svvTScbWuzDQDqpbe3N5zt6OgIZ7/3ve8dTTnAcXL99deHs2NjYxVWMr4uuuiiSrK7du0KZz/zmc+Es54VebNZunRpOHvGGWeEs9n3aciZPHlyODt37txwtr29PVXH4OBgKv9m55MeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC10DbeBfDqPvvZz4az8+fPTx1769at4ezcuXPD2YULF4azZ599djh7+umnh7MvvvhiODtjxoxwtipjY2Ph7MDAQDjb09MTzmbO765du8LZUkp59tlnU3k4WtmxeSK55pprwtne3t5KavjhD39YSRbgWHrPe94TzmbuG/YzcPzddNNN4Wyj0QhnW1pOrP/2c9++feHsoUOHwtlZs2aFsyeffHI4+/u///vh7L/+1/86nIWJat68eeHsb/7mb4azL7zwQjj71a9+NZwl74ILLhjvEjgKJ9bdHgAAAAAA4FVoegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtdA23gXw6h5//PFKslmPPPJIJcedNGlSOLtw4cJwduPGjeHs4sWLw9mqDA8Ph7Pbtm0LZ//4j/84nJ08eXI4u2PHjnAWeHXnn39+OHvdddeFs21t8Vt7f39/OPt//s//CWeHhobCWYDXM2vWrHD2tNNOC2f7+vrC2cHBwXAWeHVnnXVWOHvKKaeEs81mM5wdGxsLZ6vyzW9+M5x99NFHw9mDBw+Gs0uXLg1nr7322nA242d/9mfD2XvvvbeSGuCNysyPzs7OcPbP/uzPwln7lLzM+2CZ9TJzP6JaPukBAAAAAADUgqYHAAAAAABQC+mmx7e+9a1y7bXXlrlz55ZGo1G+9KUvHfHzZrNZVqxYUebOnVu6u7vL8uXLy2OPPXas6gUAAAAAAHhF6abH/v37y/nnn19uv/32V/z5LbfcUm699dZy++23lwcffLD09vaWq666qgwMDLzhYgEAAAAAAF5N+heZX3PNNeWaa655xZ81m81y2223lZtvvvnwL1393Oc+V+bMmVPuuuuu8hu/8RtvrFoAAAAAAIBXcUx/p8fGjRtLX19fufrqqw9/r7Ozs1x55ZXl/vvvf8X/z+DgYOnv7z/iCwAAAAAAIOuYNj36+vpKKaXMmTPniO/PmTPn8M9+0qpVq0pPT8/hr/nz5x/LkgAAAAAAgDeJY9r0eEmj0Tjiz81m82Xfe8mnP/3psnfv3sNfmzdvrqIkAAAAAACg5tK/0+O19Pb2llJ+9ImPU0455fD3t2/f/rJPf7yks7OzdHZ2HssyAAAAAACAN6Fj+kmPRYsWld7e3rJ69erD3xsaGir33Xdfufzyy4/lXwUAAAAAAHCE9Cc99u3bV55++unDf964cWNZt25dmTFjRlmwYEG58cYby8qVK8uSJUvKkiVLysqVK8ukSZPK9ddff0wL58R34MCBcHb9+vWV1PD4449XctyqXHzxxeHs5MmTw9nnn38+nP3e974XzgKvbtGiReFsW9sx/WDmYZn5vGHDhkpqAHg9Z511ViXHHRgYqOS48GYya9asVP6GG24IZ6dMmZIt55jbtWtXOLtmzZpw9ktf+lI4OzQ0FM5m7Ny5M5xdvnx5ODt16tRw9hd/8RfD2fb29nC2lFL+8R//MZwdHR1NHZt6y7zvUkop5513Xjj7wgsvhLMbN25M1UHOe9/73nC22WyGs5n3Lw8ePBjOkpd+F2XNmjXlZ37mZw7/+aabbiqllPKhD32o/Pf//t/LJz/5yXLw4MFyww03lN27d5dLL7203HPPPakbHwAAAAAAQFa66bF8+fLX7HA1Go2yYsWKsmLFijdSFwAAAAAAQMox/Z0eAAAAAAAA40XTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBaaBvvAuDNbtq0aeHsBz/4wXC20WiEs1/+8pfD2f3794ez8GbzW7/1W+HssmXLKqnhO9/5Tjj7f//v/62kBoBjad68eZUc92tf+1olx4U3k9bW1lR+ypQpFVUSt2HDhnD2M5/5TDi7b9++oyln3OzatSuc/fu///tw9ld+5VfC2a6urnD2/e9/fzhbSinf//73w9kdO3akjk29ve1tb0vlOzs7w9l77703Ww4Js2bNCmcvv/zycHZsbCyc/epXvxrOjo6OhrPk+aQHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAttI13AfBm9853vjOcnTp1aji7f//+cHbbtm3hLLzZ9PT0hLNLliwJZ9vb28PZgYGBcPbLX/5yODs4OBjOAhxLZ5xxRjj70z/90+Hspk2bwtlHH300nAUmto0bN4azf/M3fxPO7tu372jKqZ2HHnoonL3sssvC2cWLFx9NOZDW1dUVzp5++umV1XHvvfdWdmxKWb58eTibeX9t69at4ez69evDWarlkx4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALXQNt4FQB0tWbIknH3Pe95TSQ1//ud/Hs5u2bKlkhqgDj72sY+Fs1OmTKmkhu985zvh7I4dOyqpAeBYWrp0aTg7efLkcPbhhx8OZ0dGRsJZ4Nhoaanmv7v8gz/4g0qOy480Go1wNnONqzpuKaVcd9114exf/dVfpY7NiaejoyOcnT59eurY3/ve97LlUJHZs2dXclzvmZ2YfNIDAAAAAACoBU0PAAAAAACgFjQ9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAW2sa7AKij8847L5xtbW0NZx9//PFw9umnnw5n4c3mrW99azi7cOHCSmpYv359OPulL32pkhoAxsuCBQvC2WazGc6uWbPmaMoBjtLy5ctT+bGxsWoKoVIXXHBBOFvV+p4dO1/4whdSeertwIED4exzzz2XOva8efPC2cmTJ4ez+/fvT9VRV9OmTQtnL7nkkkpqePLJJys5LtXySQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqQdMDAAAAAACoBU0PAAAAAACgFjQ9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFpoG+8C4ETR1hafLueee244OzIyEs5+8YtfDGdHR0fDWaiDKVOmhLPvfe97w9nM3M/YtGlTODs4OFhJDQDHUk9PTzh75plnhrN9fX3h7Nq1a8NZ4I274IILxrsEfkxmP3zqqaeGs9dee+3RlHNMDQwMpPJjY2MVVcKJKPO+y/bt21PHvvjii8PZm266KZz9+te/nqpjvM2bNy+cnT17djg7c+bMcLbZbIazGVUdl2r5pAcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC20jXcBcKJ4z3veE84uXLgwnH3kkUfC2aeffjqchTebn//5nw9nFy9eXEkNa9euDWe/9KUvVVIDwHi54oorwtlp06aFs5m9EsCb2fve975w9p3vfGeFlcTs3LkznL3zzjtTx961a1e2HCillPLFL36xsmNfcMEF4exv/uZvVlZHFQYGBio57pQpUyo5bsa3vvWt8S6Bo+CTHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1ELbeBcA4+n8888PZ9/3vveFswcPHgxnv/KVr4SzwKv7uZ/7ufEuofzP//k/w9nBwcEKKwE4/mbNmlXJcffv31/JcQFOBDfddFM4e8opp1RYybG3devWcPapp56qsBL4/23bti2V/8xnPhPOzp8/P5ydM2dOqo7xtmbNmkqO++/+3b8LZy+77LJKahgZGankuFTLJz0AAAAAAIBa0PQAAAAAAABqQdMDAAAAAACoBU0PAAAAAACgFjQ9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGqhbbwLgGNtypQp4eyv/dqvhbMtLfEe4Q9+8INw9umnnw5ngYkts/6Mjo5WWMmxd/DgwXA289paW1vD2e7u7nA2Y/Lkyan8z/3cz1VSR8bY2Fg4+7//9/8OZ4eGho6mHCillHLBBRdUctx169ZVclzgjcs8Ix1NPurcc8+t5Li//uu/Hs6edNJJldTQaDTC2WazWUkNVfkv/+W/jHcJcFxt3ry5kmydbd++fbxLKPPmzQtnn3/++QorIcMnPQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqQdMDAAAAAACoBU0PAAAAAACgFjQ9AAAAAACAWtD0AAAAAAAAaqFtvAuAiJaWeH/ud37nd8LZWbNmhbPbt28PZ7/whS+Es0B9/OEf/uF4l1CZBx54IJzdu3dvODt16tRw9u1vf3s4y/8vcz2++tWvVlgJJ6IlS5aEs9OmTauwEmAiuvfee1P597///ZXUcdNNN4WzY2NjldRQ1XEzz8JV1ZDxzW9+c7xLAGqk0WhUks14/vnnKzku1fJJDwAAAAAAoBZSTY9Vq1aVSy65pEydOrXMnj27/MIv/ELZsGHDEZlms1lWrFhR5s6dW7q7u8vy5cvLY489dkyLBgAAAAAA+Emppsd9991XPvKRj5Tvfve7ZfXq1WVkZKRcffXVZf/+/Yczt9xyS7n11lvL7bffXh588MHS29tbrrrqqjIwMHDMiwcAAAAAAHhJ6nd6fP3rXz/iz5/97GfL7Nmzy9q1a8tP//RPl2azWW677bZy8803l+uuu66UUsrnPve5MmfOnHLXXXeV3/iN3zh2lQMAAAAAAPyYN/Q7PV76xZgzZswopZSycePG0tfXV66++urDmc7OznLllVeW+++//xWPMTg4WPr7+4/4AgAAAAAAyDrqpkez2Sw33XRTueKKK8qyZctKKaX09fWVUkqZM2fOEdk5c+Yc/tlPWrVqVenp6Tn8NX/+/KMtCQAAAAAAeBM76qbHRz/60fLwww+X//W//tfLftZoNI74c7PZfNn3XvLpT3+67N279/DX5s2bj7YkAAAAAADgTSz1Oz1e8rGPfax85StfKd/61rfKvHnzDn+/t7e3lPKjT3yccsoph7+/ffv2l3364yWdnZ2ls7PzaMoAAAAAAAA4LPVJj2azWT760Y+WL3zhC+Xee+8tixYtOuLnixYtKr29vWX16tWHvzc0NFTuu+++cvnllx+bigEAAAAAAF5B6pMeH/nIR8pdd91VvvzlL5epU6ce/j0dPT09pbu7uzQajXLjjTeWlStXliVLlpQlS5aUlStXlkmTJpXrr7++khcAAAAAAABQSrLpcccdd5RSSlm+fPkR3//sZz9b/tW/+lellFI++clPloMHD5Ybbrih7N69u1x66aXlnnvuKVOnTj0mBfPmNHv27HD2tNNOq6SGV/r9Na9mx44dldQAvLqHH344nL3wwgsrrKSe3va2t413CSmjo6PhbLPZrKyOhx56KJzduHFjJTU89dRTlRyXN4fMetnSEv8Q+aZNm8LZDRs2hLPA8bVmzZpU/pprrglnvYeQNzAwEM5u3bo1nP3sZz8bzu7duzecBXg9mWe1Kp/rOPGkmh6RwdNoNMqKFSvKihUrjrYmAAAAAACAtNTv9AAAAAAAAJioND0AAAAAAIBa0PQAAAAAAABqQdMDAAAAAACoBU0PAAAAAACgFjQ9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBbaxrsA3rxmzpwZzn7iE5+opIbPf/7z4ey6desqqQE4Nv7iL/4inH33u98dzra2th5NOcfU3Llzw9m3v/3tFVYS861vfSuc3blzZyU1rFmzJpzdtm1bJTXARNXR0RHOXnDBBZXUkJmjY2NjldQAvHG7du1K5e+4445w9sILLwxnr7766lQddfXVr341nP2nf/qnCisBODYy+9aM4eHhSo7LxOGTHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtdA23gXw5rV8+fJwdubMmZXU8MQTT1RyXGBi+9rXvjbeJVTmr/7qr8a7BGCCGx0dDWf3798fzj700EPh7D333BPOAvWxYcOGSrKPPvpoOJt5Dr3gggvC2cwa+M///M/hbKPRCGe3bt0azgKcCK644opw9sCBA+Hsl7/85aMphxOIT3oAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANRC23gXQL0sWbIknH3Xu95VYSUAALyS0dHRcPYP//APK6wE4Nh45JFHKskCML42btwYzn7jG98IZ9evX3805XAC8UkPAAAAAACgFjQ9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBaaBvvAqiXs846K5zt6uqqpIbt27eHs4ODg5XUAAAAAAAcvdtuu228S+AE5ZMeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC10DbeBUDE5s2bw9k/+ZM/CWf3799/NOUAAAAAADAB+aQHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtNJrNZnO8i/hx/f39paenpyxdurS0traOdzkAAAAAAMA4Gh0dLevXry979+4t06ZNe82sT3oAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANRC23gX8GqazWZpNpuvm2s0GuFjjo2NhbOZ42aykdd0NMet0olY83hzzjheMmNtIqhqvcwem7yJMNbqfo3dO36kqrFW9/0aMHFZ1+D4M+arZV3jeKnqOp9ox63SiVhzhE96AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC23jXcCraTQapdFoHNNjtrSMf4/nWL+m46GqmpvNZiXHzZgI12NsbCyczdQ7EV5b3VU1hl3nH8m+tsz1yGQz947McTOvL7NOZOrNZDM1ZLKjo6PhbFtbfNtyIs6NqmquavxUZSJcu4lQQ9ZEuCdVpaq1tSp1vhYcnYlw7SZCDaXk7kkZVe1pMqraw7e2toazmX1VVWvrRFmzJ8qYr8JEOMcT4fxOhBomijrP54lwnSfCeajSiVhzxPg/2QIAAAAAABwDmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC20jXcBjI/W1tZwdnR0tJIams1mODs2NlZJDRktLfEeYSbb1hafhplrkTm/meNmXls2n6m50Wik6qhCpt7seSN3fkvJjYnMGpiZo1Vd58y5GB4eDmczcz+TPXDgQDibOWft7e3hbEaVY20iyNR7oq3DdZcdm1Uct6oxkVmHJ4LMXjSzXla1x83ejzLX40Sb+9a1iaWqZ4Oq1suRkZFwtqOjI5zN7C8zMudhcHAwnK1qjciMh+z8zNQ8Ed5vmAhOtPXyRKu3lOrWqqpeX2atquo9vomwb63qfdHsfniijOMqRK9d6n24oy0GAAAAAABgIkk1Pe64445y3nnnlWnTppVp06aVyy67rPzDP/zD4Z83m82yYsWKMnfu3NLd3V2WL19eHnvssWNeNAAAAAAAwE9KNT3mzZtX/viP/7isWbOmrFmzpvzsz/5s+Rf/4l8cbmzccsst5dZbby233357efDBB0tvb2+56qqrysDAQCXFAwAAAAAAvCTV9Lj22mvLu9/97nLmmWeWM888s/zRH/1RmTJlSvnud79bms1mue2228rNN99crrvuurJs2bLyuc99rhw4cKDcddddVdUPAAAAAABQSnkDv9NjdHS03H333WX//v3lsssuKxs3bix9fX3l6quvPpzp7OwsV155Zbn//vtf9TiDg4Olv7//iC8AAAAAAICsdNPjkUceKVOmTCmdnZ3lwx/+cPniF79YzjnnnNLX11dKKWXOnDlH5OfMmXP4Z69k1apVpaen5/DX/PnzsyUBAAAAAADkmx5nnXVWWbduXfnud79bfvM3f7N86EMfKo8//vjhnzcajSPyzWbzZd/7cZ/+9KfL3r17D39t3rw5WxIAAAAAAEBpy/4fOjo6yhlnnFFKKeXiiy8uDz74YPmv//W/lk996lOllFL6+vrKKaeccji/ffv2l33648d1dnaWzs7ObBkAAAAAAABHOOrf6fGSZrNZBgcHy6JFi0pvb29ZvXr14Z8NDQ2V++67r1x++eVv9K8BAAAAAAB4TalPevze7/1eueaaa8r8+fPLwMBAufvuu8s///M/l69//eul0WiUG2+8saxcubIsWbKkLFmypKxcubJMmjSpXH/99VXVDwAAAAAAUEpJNj1eeOGF8oEPfKBs27at9PT0lPPOO698/etfL1dddVUppZRPfvKT5eDBg+WGG24ou3fvLpdeemm55557ytSpUyspvpQffdIk6rV+t8ibzejoaDibOceZ7NjYWCXZzGtra4tPgZaW+Aejurq6wtmMqs5va2vr0ZQTUtX4yWQz166q42ZUta6NjIyEs1WNiUy97e3tqWNPmTIlnH2tf3bxJy1evDicXbRoUTg7efLkcDYzn/v7+8PZLVu2hLM//vu7Xs+zzz4bzmbGe+acZcbaoUOHwtlScnMpc+0ya0pmjmbuiyeaibAPzNRQ5bEzYy0jM9Yy2cx4ryqb2Qdmzu/g4GAl2arWnqzMdZ4I8+5Eq2EivLYqVbVmVnWOOzo6wtlJkyZVks3M/YGBgXC2qvNQley6Zu5PHFW9thPtPJRS7b4xqqr7eCabmc8ToYbM/rKq9y+zdVQ1P8Z7Xcv8/ammx9/8zd+87l+8YsWKsmLFisxhAQAAAAAA3rBq/jNlAAAAAACA40zTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqoW28C3ijGo3GeJdQmdbW1nC22WxWVkd7e3sldYyNjYWzBw8eDGdHR0fD2c7OznC2o6MjnK2q3paWeJ9yaGgonO3q6gpnM+ehlNx1HhkZCWczYy2TnQhrSlU1ZMZPpobMWpUZa7NmzQpnSyllwYIF4ewll1wSzp577rnhbGZ+9Pf3h7NTp04NZ88555xKavjbv/3bcPYb3/hGOLt3795wNrOeZLJZmTGfUWXNVCczHrLXuMr9XVTmfpC5z2T2Spn9weLFi8PZSy+9NJzNrMNPPPFEOPvkk0+Gs/v37w9ns6paf6o6bp33a1XJzM9MNnuNM88omToyz6xTpkwJZ+fOnRvOZtafzL0js1fasmVLOLtnz55wNvPMeujQoXA2c42zMvfQwcHBcDZzT8qYCGtKpoaJ8Ixd1bqWVdV7E5l5l1lTMtcjs8Zn5lFGVe9NZI6beR8jI3ONszJjra7rmk96AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUQtt4FzBRNZvNcLbRaFRYSUxLS65/1dnZGc5mXt+hQ4fC2YGBgUqO29YWH9aTJk0KZ6dMmRLOtra2hrNjY2PhbOacjYyMhLPDw8PhbGbslJI7F5l5l6k5Mz9OtLmfkTkPmXnU1dUVzp588snh7KJFi8LZUkpZunRpOLtw4cJwdufOneHs+vXrw9lNmzaFs729veFsZgwvW7YsnD3ttNPC2Xnz5oWzmbnc19cXzu7bty+czazDpeTmUlXZzBzNZE80Va3DmWuRqSG7X6tKdsxHZcZaZn/Q398fzg4ODoazM2bMqCT7zDPPhLOjo6PhbOa6VTnWMnWcaPuqiVBvVcfNXLfMuMycs2w+cy4y68+sWbPC2XPPPTeczeyrMvVu3rw5nM1cu4MHD4azGzduDGf37t0bzp500knhbE9PTzhbSm6sVfX8XpWJUMNEWLMzNWTeH6lSpuZp06aFszNnzgxnJ0+eHM7u2bMnnM08N1f1Hl/m/aru7u5wNvN+YGasZd7jKyV33jL3g6pk9qNVrGsT48kLAAAAAADgDdL0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqoW28C3ijms1mONtoNCrJtrXFT2Mm29XVVclxSymlu7s7nB0aGgpnDx48GM4ODw+Hsy0t8f7c4OBgOPvss8+GsyMjI+Fs5nq0traGs5lzdujQoXC2s7MznB0dHQ1nS8mN48xYy2hvbw9nM3O/KlWta1XJzI3MWJs6dWqqjsyxt2/fHs5u3bo1nH300UfD2cceeyyczcy7/fv3h7OZNSUzPzs6OsLZzPjZt29fOJtZezI1lJKbo5lsRva+X1cTYb2cCOtwKaWMjY2Fs5m9x6RJk8LZzNzP7Bkz+8DMvSNz3xgYGAhnM/eYzLqWWbOrHJdVzbuJsK5l6q3qPGTmcmZuVFVv9tmgymNHzZkzJ5w9++yzw9nFixeHs5m9R+ZZJnPczPPic889F86+8MIL4WzmHlPVeCglt75m6sjcb080Va0pmXUtcy0y9WbHWub+NW3atHB24cKF4eypp54azmZeXybb398fzmbWquyzWtTJJ58czs6fPz+czYzhzNpaSinbtm0LZzPP75maM+M9s6epgk96AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUQtt4F/BGtbXFX0Imm9HV1RXOzpw5s5JsT09POJs9dmtrazi7d+/ecLa/vz+cPXDgQDi7a9eucHbPnj3h7NjYWDibuR6TJ08OZ4eHh8PZTL0jIyPh7KFDh8LZUkoZGBgIZzOvr9FopOoYb81mM5zNvLaWlnjvOjMmMtc5s0Zk5ufmzZvD2WwdmbH2xBNPhLNPPfVUOPvcc8+Fs6Ojo+HsmWeeGc5m6t20aVM4+/zzz4ezO3fuDGcz60nmnM2ZMyeczcrM/Uw2s25n5n5GVa+tqvU9c84ye8bM+c2sPaXkxnFG5npk9mCZejPXI2PatGnh7NSpU8PZvr6+cHbHjh3h7NDQUDjb0dERzmbPb1XXo6rnr6pUtVZl9iiZvV3mumWOW9WeqpTceZs0aVI4e+qpp4azZ599djg7f/78cLaqe0fmefH0008PZ9vb28PZzPq+e/fucLaqMVxKbn3NZDMmwjNrZqxlznFm/GRk6s2sVVXunTPvCS5atCiczTzXdXZ2hrOZZ8BnnnkmnK1q/5MZa5m9XWZ+Zt5DnT59eiU1lJJbXzPvd2buX5l5l1HFeumTHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtdA23gW8mpaWltLS8vo9mUajET5ms9kMZ7u6usLZyZMnV5KdNm1aODt//vxwtpRSFixYEM5mzsWhQ4fC2aGhoXA2c50z9Y6Ojoaz7e3t4eykSZPC2YGBgXB2165d4WxbW3x6j42NhbMPP/xwOFtKKQ888EA429raGs5G1oejkTlu5rxlrkdmXI6MjISzmXozx83Ytm1bOHvw4MHUsTNzKXOOt2/fnqojasaMGeFsZl1bunRpOJu5F2zevDmcrWqtmj59ejibkbl3lVLKlClTwtnM6xseHg5nM2O4qnUto6r9Wua4Vd03MgYHB1P5zLnI3EMz2cx+LXPczBju6ekJZ08//fRw9uSTTw5nN2zYEM729fWFs5nzm9lfZsd75r6fWdc6OztTdURl5kZV+7WMzHjP1JC5zpn5WdU6XEpuTMyZMyecfdvb3hbOnnfeeeFsZt5l5lFm35oZw5n3EBYuXBjOHjhwIJzNrIHPPvtsJTWUUt2+KjOXqlp/MsfNzNGq1sCMzLpW1VrV0dERzpaS20+cddZZ4eypp54azq5fvz6cffTRR8PZrVu3hrOZNTBzL8hkM3M5M4+6u7vD2cxzfua5uZTcHM28h5CZd9n7/nga/6c/AAAAAACAY0DTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqQdMDAAAAAACohbbxLuDVjI2NlUajEcpFtba2hrNDQ0Ph7ODgYDi7Z8+ecDajq6srlR8ZGQlnM+cic44nTZoUzmZeX2TcvGTKlCnh7EknnRTOZs5Df39/OJvR29sbzra3t4ez27ZtS9XR0hLvrWayGZnrMTo6WkkNmeNm1rXMeK9KZo04ePBgOHvo0KFUHTt27Ahne3p6wtnMuMysE5kaFixYEM6+853vDGfPOOOMcPbb3/52OJvR2dkZzmbuBc1mM5zdt29fOFtKKcPDw+Fsd3d3OJsZ85l1LbNPyawpVa0/E+G4VZ3fzHFLydXc0dERzmbWtar22m1t8ceQuXPnhrNnnnlmOJvZ/7zwwgvhbGYuZ9a1zDXO3Juzx86ct8yYyIy1ibBfq0rmPGTmcua1Ze5z2TU78/x1ySWXhLNXXHFFOHvqqaeGs1U962eeqbJ74qhZs2aFswsXLgxn3/GOd4SzmbWnr68vnC0l955H5p6UOW5mvczM/apkXlt2T1OFqta1zDpVSm5+nHbaaeFs5hll06ZN4WxmTzNt2rRwdvr06eFsZvxk7jOZ6zx16tRwNnMeMnu7TL2l5O77mTomwnpZBZ/0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqQdMDAAAAAACohbbxLuCNajab4ezo6Ggl2aGhoXB2z5494eyOHTvC2YGBgXC2lFJGRkbC2UOHDoWzHR0d4Wxra2s4mzFt2rRwtq0tPgUyx82cs0x2wYIF4eyiRYvC2cy1ePHFF8PZUko5cOBAOJsZl5maG41GOHuiyby2zHjPGBsbC2cz9Q4ODqbq6O7uDmdbWuI9/8wa39nZGc7Onj07nL3wwgvD2Xnz5oWz27dvD2cffvjhcLa/vz+cnTRpUjg7ffr0cHbv3r3hbHaNyKzb7e3t4WxVe5oTTWZ+VnXcTLaqtbWU3JqSGceZ+21V1yPz2pYsWRLOzpw5M5zN7Muff/75cDajq6srnM3M+8w1LiV3L8+sVdk6TiSZ81DVXjRz3TKqvB/Nnz8/nL344ovD2cWLF4ezmb1dZj+xefPmcHb9+vXhbGZflVHVs+UFF1wQzmb2SU888UQ4W0ruHGeuc+a+mHlmzayXmTWlqvUns05UdY8ZHh4OZzPn96STTgpnS8mta5n3lTJrSiabuR6nnnpqOJt5PzBz78i8L5AZ77NmzQpn586dG85m7jE7d+4MZ0vJ3Q+q2oNl5uh480kPAAAAAACgFjQ9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBaaBvvAupg//794ezg4GA429YWvzyHDh0KZ0spZWBgIJxtbW0NZxuNRjg7Ojoazh48eDCc7e7uDme7urrC2cx5yIyJ3t7ecPbcc88NZzs7O8PZZ599Npx96qmnwtlScmOz2WyGs5mxNjIyMu41ZLS0VNOPzqwpmRqGhobC2UmTJoWzWSeddFI4mxmXL774YjibOW/z588PZ88///xwtr+/P5xdt25dOPvYY49VUkPmnGWOm8keOHAgnC2llI6OjnA2M9YyczRzD61qrarK2NhYOFvVelnVOcvsO0rJvb7MOK7qHA8PD4ezmXOc2StlbNu2LZzt6+sLZ6vaX2b2w5lrnK0js/5knGhr1USoN3PdMtrb2yvJllLKkiVLwtlFixaFs5n77datW8PZxx9/PJx94oknwtlnnnkmnM3M/SlTpoSzmX1HZp99zjnnhLOTJ0+uJFtKKXv27Alnd+3aFc5m5t1EeLasSuY+kzkPmXtM5jk0s3/P7jvmzp0bzmbeE8y895LZp2TOcWZvl8lmrl3m/cvMtcvcYzLP7mvXrg1nn3/++XC2lNy5yNyfM/eDzHwebz7pAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALbSNdwHH09jYWCXZjNbW1nC2o6MjnG02m0dTTkhbW3yYNBqNcLa9vf1oynldU6ZMCWczr21gYCCcHRkZCWcXLVoUzi5dujScHRoaCmcfe+yxcPa5554LZ0spZXh4OJzNjInMOc5c5zrLrBNVzc+MyZMnp/JdXV3h7KFDh8LZ7u7ucPb0008PZ5ctWxbOZq7d97///UqyP/zhD8PZzs7OcHZ0dDSc3blzZzibWXuy9/xp06aFs5lrl7mHZta1qmqYCDLXrqUl/t/6ZMZlpobs/Shzr6tqn5vZj+7bty+c7enpCWdPOeWUcDYzhrds2RLOZtaUSZMmhbOZ+ZkZD1XO5UzNmbk0EfYeVanqnFX1nJap4dRTTw1nS8ntfzL322effTacfeihh8LZNWvWhLNPP/10OPviiy+Gs5k1O7O2Zta1zDN25vl27ty54WzmHlNKbvxkVLVHqPN+LfPaqnpvKzMeMmO4lFJ6e3vD2cz7KZs2bQpnDxw4EM5m7reDg4PhbOae1N/fH85mXltmTcm8v5bZD2eu8Y4dO8LZUnLrT+b9kbrySQ8AAAAAAKAW3lDTY9WqVaXRaJQbb7zx8PeazWZZsWJFmTt3bunu7i7Lly9P/VfkAAAAAAAAR+Oomx4PPvhgufPOO8t55513xPdvueWWcuutt5bbb7+9PPjgg6W3t7dcddVVqX8aCAAAAAAAIOuomh779u0rv/qrv1r++q//ukyfPv3w95vNZrntttvKzTffXK677rqybNmy8rnPfa4cOHCg3HXXXcesaAAAAAAAgJ90VE2Pj3zkI+U973lPede73nXE9zdu3Fj6+vrK1Vdfffh7nZ2d5corryz333//Kx5rcHCw9Pf3H/EFAAAAAACQ1Zb9P9x9991l7dq1Zc2aNS/7WV9fXymllDlz5hzx/Tlz5pRNmza94vFWrVpV/vN//s/ZMgAAAAAAAI6Q+qTH5s2by8c//vHyd3/3d6Wrq+tVc41G44g/N5vNl33vJZ/+9KfL3r17D39t3rw5UxIAAAAAAEApJflJj7Vr15bt27eXiy666PD3RkdHy7e+9a1y++23lw0bNpRSfvSJj1NOOeVwZvv27S/79MdLOjs7S2dn59HUDgAAAAAAcFjqkx7vfOc7yyOPPFLWrVt3+Oviiy8uv/qrv1rWrVtXFi9eXHp7e8vq1asP/3+GhobKfffdVy6//PJjXjwAAAAAAMBLUp/0mDp1alm2bNkR35s8eXKZOXPm4e/feOONZeXKlWXJkiVlyZIlZeXKlWXSpEnl+uuvP3ZVAwAAAAAA/IT0LzJ/PZ/85CfLwYMHyw033FB2795dLr300nLPPfeUqVOnHuu/qpTy8t8fcqy0tMQ/BJP557m6u7vD2df6vSk/aXh4OJzN1pGROW9tbfHhN2nSpHA2c97GxsbC2YMHD4az06dPD2cvuOCCcDYzj5599tlw9vHHHw9nd+/eHc6WUkpra2s4m5lLmWuX0Ww2w9nM+lPVcTMyNWSuW6bejo6OcDYz50opZWRkJJydPHlyOHvhhReGs+94xzvC2UWLFoWzGzduDGdf+qcmI9auXRvO7t+/P5zN3GMOHDgQzmbGZWb8ZPco7e3t4eyhQ4fC2cy6lqn5RFvXJsJxM+tJVTWU8qNPTFdRR2ZfldnbZebomWeeGc7++D+b+3oye+LM2po5bmaPOzo6Gs5WdS2qlB3z463O+7XMPSazJ8/Mz2y+v78/nF2/fn04+73vfS+czawTO3bsCGcz1zmzTuzatSuczeyVTj755HD23HPPDWcXLFgQzmb276VU995L9hklaiLsf6rc01RRQ+a++Gr/5P4ryYzLUnJrZl9fXzi7ffv2cDZzP5gyZUolx808L+7bty+cnTVrVjh71llnhbOZMbFz585w9rnnngtnM8/CpeTGfFX3mcx4H+991RtuevzzP//zEX9uNBplxYoVZcWKFW/00AAAAAAAAGGp3+kBAAAAAAAwUWl6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAttI13AcdTo9EY9+N2dHRUks0aGRkJZzOvb2xsLJwdGhoKZzP2798fzo6OjoazzWYznD3jjDPC2dNPPz2czby2devWhbObN28OZ7Pa2uLLTEtLvA9b1XzOyIyJiVBvRuZadHV1hbMHDhwIZ/fs2RPOllJKd3d3OHvWWWeFs8uXLw9nzz///HB2YGAgnH322WfD2aeeeiqc3bhxYzg7efLkcHbevHnhbOZ+dOjQoXB21qxZ4eykSZPC2VJy9+ft27eHs5n7YmZNyWQz1yOzvmdUtV5m1uyqZO7jWZk1sLOzM5wdHBwMZzNjIrNXytxnMmvgo48+Gs5m5kZmf5m531Y150rJ7eEz2czry6hqPk+E/VpV63vmumXG2uzZs8PZUnLzOXMP3bBhQzi7fv36cHbXrl3hbGadaG1tDWermkdVvt8QlRlrw8PDqWO3t7eHs5n7YlXvY1Q1n6saPxNhvcxc48x+P7OnKqWUgwcPhrNbt24NZ3fu3BnOZp6dM9cu8/yemRuZOZfZM5533nnhbGZMPPDAA+Hs888/H85m9tml5MZ85p6UWScy+9zx5pMeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC10DbeBUxUjUYjnG1tbQ1nx8bGwtmhoaFKaigl9/pGRkbC2c7OznC2vb09nM2ct/7+/nB2cHAwnD3llFPC2XPPPTecnTp1ajj7+OOPh7NPPPFEOLtnz55wtqOjI5w9mnxUZgxntLTUtxc8Eda1Q4cOhbOZ+VlK7vVl5vPSpUvD2enTp4ezjz32WDi7efPmcHb37t3hbEbmnJ188snh7MGDB8PZzLhsNpvhbFdXVzhbSm6s7d+/P3XsqMy5GB0dreS4Vclcu8y1yKxVGZl9UuZalFJKW1t8q555fZlznDluZg2cP39+OJtZJ37wgx+Es9u3bw9nM/u1jKquW3afVNVYq2reZV5f5rxVpap6M2tKZl+VGQ+ZeV9K7vVl5ujOnTvD2cx+P/N8u2/fvnA286w/a9ascLa3tzecXbhwYTh72mmnhbNTpkwJZzPPBtk9Veb+nHlvIpOt6tmyqrX1RJNZTzLZzPwspbr9fmbud3d3h7OZOTp58uRwtqenJ5ydOXNmOHv66aeHs5ln1sx1fvrpp8PZvXv3hrOZ+20p1b2/VpWq3reLqu+7ewAAAAAAwJuKpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAtt413AG9XSEu/bjI6OhrOtra1HU87rGhoaquS4IyMjqfzY2Fg429YWHyaNRiOczdR86NChSrJdXV3h7Nlnnx3OnnPOOeFspt4NGzaEs319feHs5MmTw9ms9vb2cDYzLjNZfqTZbIazmfUyM5cz123KlCnhbCmlLFy4MJxdvHhxOJtZJ374wx+Gs6tXrw5nn3nmmXA2sw5Pnz49nM2sVTt27AhnM9e5o6MjnK1yjRgcHAxnM3Mpc7/NyOxpsvuJKmTGcEZmz5jZr2XW1iplXl9mfmTOxZIlS8LZmTNnhrOZ9efpp58OZzPr+7Rp08LZzFw+ePBgOJsZa9lnmarGT1XzOWMirClVnbPMcYeHh8PZnp6ecHbWrFnhbCm5Z4PMPSkzPzI1ZNaJzs7OcLa7uzucnTp1aji7YMGCcPass84KZzP77Ey9mTUws78spZQ9e/aEs5n7TFVztKr3oCaCzDnLrK2ZdW3//v3h7NatW8PZUkqZN29eOJuZS5n1J/OclFm3M8+LmTVw+/bt4WzmeTHz3taLL74YzmbqzewDM+tl9tiZ9TUjs65l5nMVfNIDAAAAAACoBU0PAAAAAACgFjQ9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAW2sa7gFfTbDZLs9l83dzY2Fj4mI1GI5zNHHciGBkZSeUPHTpUSR1dXV3hbFtbfPhFxsJLMtd54cKF4ezFF18czs6cOTOcXb9+fTj7zDPPhLOZc9be3h7OThSZ1zc6OhrOtra2hrOZsVaVzHnIrGsHDx4MZzPnNzPWFi1aFM6WUsrb3va2cHbZsmXh7IEDB8LZBx54IJy9//77w9nMGn/qqaeGsy0t8f/2YWBgoJJsb29vODs8PBzOTpS53NHREc5mas7I3G8z8/lEU9XeLnN+s2Mtc+wpU6aEs5l1bWhoKJydO3duODtt2rRw9oUXXghn9+7dG85Onjw5nM3scTNrdmZMdHZ2hrPZ8Z7ZT1SVrfO+aiLUkNmDzZ49O5ydMWNGOJutoyqZtTWT7enpCWcz8zmzt1uwYEE4m3kWnjdvXjibmcuZ9X3Dhg3hbCmlbN68OZzNvD+SGROZOVrVenmircOZ55OMzP4g875LKblnlMx+7YwzzghnM/uqzFqVeT7JzOcXX3wxnM3Um7nH7N69O5zdv39/OJtZ3zP7y1Kq22NmVPU+fBX1+qQHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAttI13Aa+m0WiURqMxbn9/s9ms5LgtLfE+09jYWCXZUkoZGRkJZ7u6usLZSZMmhbOdnZ3h7NDQUDjb09MTzp533nnh7KJFi8LZvXv3hrPr1q0LZ7dv3x7OZsZwVeMyKzPnM9lMza2treHsiSYz7zPZtrb4rSRzfufPnx/OllLKggULwtkpU6aEs5m59Pzzz4ezW7ZsCWc7OjrC2enTp4ezg4OD4Wx3d3c4m7lvZOby/v37w9lp06aFs5MnTw5nS8m9voMHD457lh/J3Osya1Vmjcjubau612XmUmbu9/b2hrOZPWNfX184u2fPnnA2c50zcy5zD82Mn8w1zj7LVDWOx/N57mhMhHpHR0fD2cwerL29PZzN7DuyYy3zXJd5XsysVZlzkTF16tRwdt68eeHsWWedFc5eeOGF4expp50WzmbO744dO8LZxx9/PJx97LHHwtlSSnnhhRfC2cweISMzn6salxNhXatK5rXt27cvnM08p5WS2yNkniMya2Bm/GTG5YEDB8LZzHnLHPfkk08OZzPnLLNfq2oPlr2HDg8Ph7NVvbeVGT/jzSc9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqoW28Cziems1mODs6OhrOtra2hrNjY2PhbEaj0UjlOzo6wtm2tvgw6erqCmdHRkYqyS5durSSbOa1rVmzJpxdu3ZtOLtnz55wdtKkSeHs4OBgOJuZR6WU0tIS761mxlrmuJls5vVl5l1Vx83IzKOMqq5bZ2dnqo7u7u5wNjM/MveDjJkzZ4az8+fPD2dPP/30cDZzT2pvb68km1nXMuNn+vTp4WzmWpSSGz87duwIZzdv3hzObt26NZw9dOhQOHuiyaytVe3BMsfNrJdZ+/btC2f3798fzp5xxhnh7OLFi8PZzF70mWeeCWcz52HKlCnhbOacVTUmMmtr9t41EfYp5GWeQzNjor+/P5zdvn17OFtKKb29veFs5v58zjnnpOqIyqwps2bNCmcz+7Vly5aFswsWLAhnM/uqzH5t06ZN4ewPfvCDcDaz9ymllOHh4XA286yfWeMzc7TOz6FVyZzfzHsemTWwlNxYy8i8vsxalXlfIDN+MnM/swerav88NDQUzmbuoZlnr+zzSWbdrmo+Z8blePNJDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqQdMDAAAAAACoBU0PAAAAAACgFjQ9AAAAAACAWmgb7wLeqLGxsUqO29Y2/qem2WyGs+3t7aljd3Z2hrPd3d3hbOZ67NmzJ5w96aSTwtnzzz8/nD3llFPC2b6+vnB2zZo14ez27dvD2ZaWeJ+yqrmRlZlLIyMj415DZt5VJXPtMmMikx0aGgpnM+fs0KFD4WxmbpRSyr59+8LZefPmhbOZ9efd7353ODt16tRwdsmSJeFso9EIZzNzI3OfyczlvXv3hrOTJ08OZ2fMmBHOzp49O5wtJVfzD37wg3D2hRdeCGcz1zlzPTLHzcisE5kaqqq3Ktl6M/eDAwcOhLNdXV3h7JlnnhnOnnbaaeFsZh/4zDPPhLOZ+8y0adPC2apk7s2jo6PhbJX7mcyxM9nMuZgIqtqvdXR0hLNVrYGZ554nnngidez58+eHs0uXLg1nTz755HD2rLPOCmcHBgYqqaGnpyecnTlzZjibGRM7duwIZx955JFw9rvf/W44u3nz5nA2+6xY1TNgpo7MfK5q3c6Micy6ljlua2trOJtR1fNJ5lm4lOpeX2ZPMzg4GM5WdZ0z5y3zXJepIbMfzpzfzGvbv39/OJud95k1JSMzhk+k568Ta2cJAAAAAADwKjQ9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBaaBvvAt6oRqMx3iWUZrNZSTajrS13KadMmRLOjoyMhLO7du0KZzM1X3LJJeHs+eefH852dHSEs0888UQ4u2XLlnB2dHQ0nG1tbQ1nW1riPc3MPMrOuarmaOb1jY2NhbNV1TsR1qqMrq6ucDYzLjPX4oUXXghnSynlqaeeCmcfffTRcPYtb3lLODt//vxw9uqrrw5n58yZE85m1veBgYFwNnM9Mve6GTNmhLMnn3xyJcfds2dPOFtKKTt37qwku2/fvnD20KFD4eyJtv5MBJlzlt2DZWT2CJn1dfbs2eHsWWedFc5OmzYtnM3sqzJr1eTJk8PZzHqZOb+Z+TlRZMZ8Zo3P7Nf4kYmwZu/fvz+czey/SinllFNOCWcz9/158+aFs7NmzQpnM+twe3t7JcfN7A82b94czq5Zs6aSbGZf1d/fH85m77eZfFXv02RMhLlfVQ2Ze2hV73lkHDhwIJXPvL7MWMusxUNDQ+Fsd3d3OJuZR5MmTQpne3p6wtnM2jo8PBzOZvaXg4ODldSQeW1ZVb0neCKxCwUAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqQdMDAAAAAACoBU0PAAAAAACgFjQ9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqIW28S7gjWo0GuHs2NhYJcetKjsyMlJJNuvQoUPhbOYcL1q0KJy9+OKLw9nZs2eHs88880w4+/TTT4ezO3fuDGdbW1vD2c7OznC2Km1t1S0bLS3xPmxmrGWymRqqUtU6kTlu5jp3dHSEs5l69+3bF86WUspDDz0Uzu7duzecPeOMM8LZzBzNzP0ZM2aEs729veHsgQMHwtnM9RgaGgpnM2Pi4MGD4ewjjzwSzm7ZsiWcLaWUrVu3hrM7duwIZwcGBsLZzLo2EWTWn6pk1veq7gXZ6zY6OhrOZmrOrBPTp08PZ1988cVw9tlnnw1nM+dh6tSp4Wx7e3s4Ozg4GM5OhH1H5h5TSnXPSRnNZnPca8jIXLvMa8uM96r2a5n7+LZt28LZUkr5zne+E85mzsXb3va2cDazt8uct8xeKbMXffjhh8PZ733ve+HsD3/4w3B29+7d4ezw8HA4m5kbVcqsmSfaHiwjs7ZWde2qWgMz1zhTQym598wy5y3znJRZqzLnIjMmMs+sPT094WxG5nmxqvdRq5xHmXxV+9ETab82/u/uAQAAAAAAHAOaHgAAAAAAQC2kmh4rVqwojUbjiK8f/3h8s9ksK1asKHPnzi3d3d1l+fLl5bHHHjvmRQMAAAAAAPyk9Cc93vKWt5Rt27Yd/vrxfzP7lltuKbfeemu5/fbby4MPPlh6e3vLVVddlfo3qgEAAAAAAI5G+jcSt7W1veIvP2w2m+W2224rN998c7nuuutKKaV87nOfK3PmzCl33XVX+Y3f+I1XPN7g4OARvxiwv78/WxIAAAAAAED+kx5PPfVUmTt3blm0aFH55V/+5fLMM8+UUkrZuHFj6evrK1dfffXhbGdnZ7nyyivL/fff/6rHW7VqVenp6Tn8NX/+/KN4GQAAAAAAwJtdqulx6aWXlv/xP/5H+cY3vlH++q//uvT19ZXLL7+87Nq1q/T19ZVSSpkzZ84R/585c+Yc/tkr+fSnP1327t17+Gvz5s1H8TIAAAAAAIA3u9Q/b3XNNdcc/t/nnntuueyyy8rpp59ePve5z5W3v/3tpZRSGo3GEf+fZrP5su/9uM7OztLZ2ZkpAwAAAAAA4GXS/7zVj5s8eXI599xzy1NPPXX493z85Kc6tm/f/rJPfwAAAAAAABxrb6jpMTg4WNavX19OOeWUsmjRotLb21tWr159+OdDQ0PlvvvuK5dffvkbLhQAAAAAAOC1pP55q0984hPl2muvLQsWLCjbt28vf/iHf1j6+/vLhz70odJoNMqNN95YVq5cWZYsWVKWLFlSVq5cWSZNmlSuv/76quoHAAAAAAAopSSbHs8//3z5lV/5lbJz585y8sknl7e//e3lu9/9blm4cGEppZRPfvKT5eDBg+WGG24ou3fvLpdeemm55557ytSpU9OFNZvN0mw2Xzf3Wr8v5Ce1tMQ/2BL5u1/S2toazmbqbWuLX55MvaWUsnPnznB2ZGQknJ0+fXo4u2zZsnB28eLF4eyePXvC2e9///vh7BNPPBHO7tu3L5zt6uoKZzNjOCNz3LGxsUpqKKWU0dHRcLaquX+iyaw/mbk8PDwczmbWqszvcMrUW0op27ZtqyT7jW98I5wdGhoKZzPj8rTTTqskm5lze/fuDWcz69rR7BEiDh06FM5mXlsppRw8eDCcrWrNzKyB/EjmWmTmZ2Ydzo6HzP4usxZn1vjMetnf3x/OPvnkk+Hs7t27w9nM3Ni/f384m7knZcbPibhHyYzLzPWo87qWeW2ZdaKq/VpmXGbmUSmlbNq0KZzNvL6tW7eGsy+9fxGRWeMzz6FbtmwJZ5955plwNrMOZ9a1zLis8nkxYyI8O2dMlPMWlVnXqrpvZGSeQwcHB1PHzuQ7OjrC2cwzVUZm7k+ZMiWczdxnMuM986z24osvhrOZZ7rMa6vy2SDz/J6pOeNE2q+lzsDdd9/9mj9vNBplxYoVZcWKFW+kJgAAAAAAgLQT7z8tAgAAAAAAeAWaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUgqYHAAAAAABQC23jXcCraTQapdFojNvf32w2w9mxsbFKamhri1+e4eHh1LEPHDgQzk6ZMiWcPeecc8LZSy+9NJzt7u4OZ9etWxfOPvroo+HswYMHw9murq5Ksq2treFsZgxnjI6OpvKZeTyec/5EVdX5zYy1Q4cOhbNVyqwTmfX1lFNOCWcz523Xrl3h7PPPPx/OPvvss+FsR0dHONvZ2RnOnnTSSeFs5jxk1p/MfXFkZCScLaXe61rm3jERXltmD9bSEv9vfTJjoqpsNp+ZH5s2bQpnd+7cWUkNu3fvruS4mX1rVeMnc4+paj3JjrWMiTD366yqcTk4OBjOZu4FmeOWUsrQ0FA4m3n+yuwnfvCDH4SzmXr37dsXzmauXUZmvcysVZl6q3p/pKrjlpJb16qso64y4zIz1jLzM3ONs8+3mXtuZvxMnTo1nK3q3tze3h7OZu4dmWu3ZcuWSmp47rnnwtnMe6iZa1zV+3ZZE+EZMFpDplaf9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqQdMDAAAAAACoBU0PAAAAAACgFjQ9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqIW28S5gomppifeDMtmxsbFwdmhoqJLjllJKW1v80p900knh7MKFC8PZqVOnhrO7d+8OZ/v6+sLZZrMZzk6aNCmczYyJjNHR0UqOW2UNmbGWuR6NRiNVx4mkqvOQuRYZmeNm1rXsNc6ct5GRkXD24MGDqTqq0N3dHc5mrkdXV1c429nZGc5mrl1mTAwPD1dSQ3astbe3h7OZsTYRnGhra+Z+m1kjMqq8N2fGWlXrxP79+8PZQ4cOhbOZsTZlypRwtrW1NZzNqOrePBH2dkwsVe3XMuMyc7/N7A+yMvN5cHAwnK1qP5G5dpnXlll/MjVk3kPI1Js5buY+nt1TdXR0hLPW4h+ZCM+hVe3XMvO+qhpKyc2PzHNoZi5l5kamhsw6vHfv3nA288z6wgsvhLM7d+4MZ/v7+8PZjOy+dSLsEaoSrSFTq096AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFALmh4AAAAAAEAtaHoAAAAAAAC1oOkBAAAAAADUQtt4F/BqxsbGSqPReN1cS8v4921GR0fD2chrOhptbblLOWnSpHB2+vTp4Wxra2s4u2XLlnB2165d4ezu3bvD2WazGc52dnZWctzBwcFwdmRkJJzNzI0q51HmXFQ1P6qSeW0ZmfNQVQ1VGRsbqyRbSilDQ0PhbGbMV3WODx06FM5m1taurq5KshlVXefM3Mhc4+xYq2otztZRhYmwrlWlqhpOtGuclRkTmbUqk83I7MurWn8ya0RGpobsWMsceyLM54wTbV2bCPVmni2rvIdWZSKsxZm1aiIYHh4OZzPnt8pn1qrW+IlgIqwTGRPhmTUzftrb2ys7dmb/k3kPqqoaMjLP45kaXnzxxXB279694ez+/fvD2cwY7ujoCGezcy5z3uq6rmWuxfh3DAAAAAAAAI4BTQ8AAAAAAKAWND0AAAAAAIBa0PQAAAAAAABqQdMDAAAAAACoBU0PAAAAAACgFjQ9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBbaxruAV9PS0lJaWk6Mnkyj0ajkuFW+/qGhoXB206ZN4ezWrVvD2cx5GxsbqyQ7MjIy7jVkVDUmms1mZTVUNT8yNVdVQ1XHrcpEqLe1tTWcbWvL3aLa29vD2Ykwfjo7O8e9hsy9IFPD6Ojo0ZTzujLXrap1OCtzLjLZzPyYCOO9zjL3xYmytx0eHq7kuJk1fiKMtYmwr6rKRBlrE2H9mQhjbSKch4xMDZl5X0p+fxc1Ee77mXmXqTdzPTLZKp8Bq6ihyrmRqaOq/VrGRFgnJsq1i8quVRlVXefM+1UZVc3nqs7DwYMHw9nM2poZw1W9tuyesaq5lDlvVY2f6GvLnIOJsRsGAAAAAAB4gzQ9AAAAAACAWtD0AAAAAAAAakHTAwAAAAAAqAVNDwAAAAAAoBY0PQAAAAAAgFrQ9AAAAAAAAGpB0wMAAAAAAKgFTQ8AAAAAAKAW2sa7gJ/UbDZLKaWMjo6OcyW85KVrEtHSEu+jNRqNcHZsbKySbGacVVVDRuZaZGSuRbaGqmrOHDfz+k40J9p5qLLezLGrymZk1onMuahqPmeOW9U9/EQb76Xkas6ct4lwnTlxnWj35hNtXE6E/dpEYf35kRPtPFQ1hkup7vVV9fyVkTlvdd4HVvXasqo6F1Xt10401rXqTYT92kRQ1do6Ec7DRHl/7UQ6by+twZE6JlzTY2BgoJRSypNPPjnOlQAAAAAAABPFwMBA6enpec1MozneLZqfMDY2VrZu3VqmTp16RDe4v7+/zJ8/v2zevLlMmzZtHCuENwdzDo4/8w6OL3MOjj/zDo4vcw6OP/MOqtFsNsvAwECZO3fu6/5rQxPukx4tLS1l3rx5r/rzadOmWTDgODLn4Pgz7+D4Mufg+DPv4Pgy5+D4M+/g2Hu9T3i8xC8yBwAAAAAAakHTAwAAAAAAqIUTpunR2dlZfv/3f790dnaOdynwpmDOwfFn3sHxZc7B8WfewfFlzsHxZ97B+Jtwv8gcAAAAAADgaJwwn/QAAAAAAAB4LZoeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAsnRNPjM5/5TFm0aFHp6uoqF110Ufl//+//jXdJUBurVq0ql1xySZk6dWqZPXt2+YVf+IWyYcOGIzLNZrOsWLGizJ07t3R3d5fly5eXxx57bJwqhnpZtWpVaTQa5cYbbzz8PXMOjq0tW7aUX/u1XyszZ84skyZNKhdccEFZu3bt4Z+bc3BsjYyMlP/wH/5DWbRoUenu7i6LFy8uf/AHf1DGxsYOZ8w7OHrf+ta3yrXXXlvmzp1bGo1G+dKXvnTEzyPza3BwsHzsYx8rs2bNKpMnTy7ve9/7yvPPP38cXwWcWF5r3g0PD5dPfepT5dxzzy2TJ08uc+fOLR/84AfL1q1bjziGeQfHz4Rvenz+858vN954Y7n55pvLQw89VN7xjneUa665pjz33HPjXRrUwn333Vc+8pGPlO9+97tl9erVZWRkpFx99dVl//79hzO33HJLufXWW8vtt99eHnzwwdLb21uuuuqqMjAwMI6Vw4nvwQcfLHfeeWc577zzjvi+OQfHzu7du8tP/dRPlfb29vIP//AP5fHHHy9/9md/Vk466aTDGXMOjq0/+ZM/KX/5l39Zbr/99rJ+/fpyyy23lD/90z8tf/EXf3E4Y97B0du/f385//zzy+233/6KP4/MrxtvvLF88YtfLHfffXf59re/Xfbt21fe+973ltHR0eP1MuCE8lrz7sCBA+X73/9++Y//8T+W73//++ULX/hCefLJJ8v73ve+I3LmHRxHzQnubW97W/PDH/7wEd87++yzm7/7u787ThVBvW3fvr1ZSmned999zWaz2RwbG2v29vY2//iP//hw5tChQ82enp7mX/7lX45XmXDCGxgYaC5ZsqS5evXq5pVXXtn8+Mc/3mw2zTk41j71qU81r7jiilf9uTkHx9573vOe5q//+q8f8b3rrruu+Wu/9mvNZtO8g2OplNL84he/ePjPkfm1Z8+eZnt7e/Puu+8+nNmyZUuzpaWl+fWvf/241Q4nqp+cd6/kgQceaJZSmps2bWo2m+YdHG8T+pMeQ0NDZe3ateXqq68+4vtXX311uf/++8epKqi3vXv3llJKmTFjRimllI0bN5a+vr4j5mFnZ2e58sorzUN4Az7ykY+U97znPeVd73rXEd835+DY+spXvlIuvvji8ou/+Itl9uzZ5a1vfWv567/+68M/N+fg2LviiivKP/3TP5Unn3yylFLKD37wg/Ltb3+7vPvd7y6lmHdQpcj8Wrt2bRkeHj4iM3fu3LJs2TJzEI6RvXv3lkajcfjTxeYdHF9t413Aa9m5c2cZHR0tc+bMOeL7c+bMKX19feNUFdRXs9ksN910U7niiivKsmXLSinl8Fx7pXm4adOm414j1MHdd99d1q5dW9asWfOyn5lzcGw988wz5Y477ig33XRT+b3f+73ywAMPlN/6rd8qnZ2d5YMf/KA5BxX41Kc+Vfbu3VvOPvvs0traWkZHR8sf/dEflV/5lV8ppbjXQZUi86uvr690dHSU6dOnvyzjvRZ44w4dOlR+93d/t1x//fVl2rRppRTzDo63Cd30eEmj0Tjiz81m82XfA964j370o+Xhhx8u3/72t1/2M/MQjo3NmzeXj3/84+Wee+4pXV1dr5oz5+DYGBsbKxdffHFZuXJlKaWUt771reWxxx4rd9xxR/ngBz94OGfOwbHz+c9/vvzt3/5tueuuu8pb3vKWsm7dunLjjTeWuXPnlg996EOHc+YdVOdo5pc5CG/c8PBw+eVf/uUyNjZWPvOZz7xu3ryDakzof95q1qxZpbW19WUdz+3bt7/sv1oA3piPfexj5Stf+Ur55je/WebNm3f4+729vaWUYh7CMbJ27dqyffv2ctFFF5W2trbS1tZW7rvvvvLnf/7npa2t7fC8Mufg2DjllFPKOeecc8T3li5dWp577rlSivscVOHf//t/X373d3+3/PIv/3I599xzywc+8IHy27/922XVqlWlFPMOqhSZX729vWVoaKjs3r37VTNA3vDwcHn/+99fNm7cWFavXn34Ux6lmHdwvE3opkdHR0e56KKLyurVq4/4/urVq8vll18+TlVBvTSbzfLRj360fOELXyj33ntvWbRo0RE/X7RoUent7T1iHg4NDZX77rvPPISj8M53vrM88sgjZd26dYe/Lr744vKrv/qrZd26dWXx4sXmHBxDP/VTP1U2bNhwxPeefPLJsnDhwlKK+xxU4cCBA6Wl5chHzdbW1jI2NlZKMe+gSpH5ddFFF5X29vYjMtu2bSuPPvqoOQhH6aWGx1NPPVX+8R//scycOfOIn5t3cHxN+H/e6qabbiof+MAHysUXX1wuu+yycuedd5bnnnuufPjDHx7v0qAWPvKRj5S77rqrfPnLXy5Tp049/F8E9fT0lO7u7tJoNMqNN95YVq5cWZYsWVKWLFlSVq5cWSZNmlSuv/76ca4eTjxTp049/DtzXjJ58uQyc+bMw9835+DY+e3f/u1y+eWXl5UrV5b3v//95YEHHih33nlnufPOO0spxX0OKnDttdeWP/qjPyoLFiwob3nLW8pDDz1Ubr311vLrv/7rpRTzDt6offv2laeffvrwnzdu3FjWrVtXZsyYURYsWPC686unp6f8m3/zb8rv/M7vlJkzZ5YZM2aUT3ziE+Xcc88t73rXu8brZcGE9lrzbu7cueVf/st/Wb7//e+Xv//7vy+jo6OH31uZMWNG6ejoMO/geGueAP7bf/tvzYULFzY7OjqaF154YfO+++4b75KgNkopr/j12c9+9nBmbGys+fu///vN3t7eZmdnZ/Onf/qnm4888sj4FQ01c+WVVzY//vGPH/6zOQfH1le/+tXmsmXLmp2dnc2zzz67eeeddx7xc3MOjq3+/v7mxz/+8eaCBQuaXV1dzcWLFzdvvvnm5uDg4OGMeQdH75vf/OYrPsN96EMfajabsfl18ODB5kc/+tHmjBkzmt3d3c33vve9zeeee24cXg2cGF5r3m3cuPFV31v55je/efgY5h0cP41ms9k8nk0WAAAAAACAKkzo3+kBAAAAAAAQpekBAAAAAADUgqYHAAAAAABQC5oeAAAAAABALWh6AAAAAAAAtaDpAQAAAAAA1IKmBwAAAAAAUAuaHgAAAAAAQC1oegAAAAAAALWg6QEAAAAAANSCpgcAAAAAAFAL/x/Bs1eEK0HEuQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 2000x5000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# visualize test data reconstructions\n",
    "def vis_reconstruction(model, randomize=False):\n",
    "  # download MNIST test set + build Dataset object\n",
    "  mnist_test = torchvision.datasets.MNIST(root='./data', \n",
    "                                          train=False, \n",
    "                                          download=True, \n",
    "                                          transform=torchvision.transforms.ToTensor())\n",
    "  model.eval()      # set model in evalidation mode (eg freeze batchnorm params)\n",
    "  num_samples = 5\n",
    "  if randomize:\n",
    "    sample_idxs = np.random.randint(low=0,high=len(mnist_test), size=num_samples)\n",
    "  else:\n",
    "    sample_idxs = list(range(num_samples))\n",
    "\n",
    "  input_imgs, test_reconstructions = [], []\n",
    "  for idx in sample_idxs:\n",
    "    sample = mnist_test[idx]\n",
    "    input_img = np.asarray(sample[0])\n",
    "    input_flat = input_img.reshape(784)\n",
    "    reconstruction = model.reconstruct(torch.tensor(input_flat, device=device))\n",
    "    \n",
    "    input_imgs.append(input_img[0])\n",
    "    test_reconstructions.append(reconstruction[0].data.cpu().numpy())\n",
    "    # print(f'{input_img[0].shape=}\\t{reconstruction.shape}')\n",
    "\n",
    "  fig = plt.figure(figsize = (20, 50))   \n",
    "  ax1 = plt.subplot(111)\n",
    "  ax1.imshow(np.concatenate([np.concatenate(input_imgs, axis=1),\n",
    "                            np.concatenate(test_reconstructions, axis=1)], axis=0), cmap='gray')\n",
    "  plt.show()\n",
    "\n",
    "vis_reconstruction(ae_model, randomize=False) # set randomize to False for debugging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7UO6153nWmC"
   },
   "source": [
    "## Sampling from the Auto-Encoder [2pt]\n",
    "\n",
    "To test whether the auto-encoder is useful as a generative model, we can use it like any other generative model: draw embedding samples from a prior distribution and decode them through the decoder network. We will choose a unit Gaussian prior to allow for easy comparison to the VAE later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsWgAMfEn3Qk"
   },
   "outputs": [],
   "source": [
    "# we will sample N embeddings, then decode and visualize them\n",
    "def vis_samples(model):\n",
    "  ####################################### TODO #######################################\n",
    "  # Prob1-3 Sample embeddings from a diagonal unit Gaussian distribution and decode them     #\n",
    "  # using the model.                                                                 #\n",
    "  # HINT: The sampled embeddings should have shape [batch_size, nz]. Diagonal unit   #\n",
    "  #       Gaussians have mean 0 and a covariance matrix with ones on the diagonal    #\n",
    "  #       and zeros everywhere else.                                                 #\n",
    "  # HINT: If you are unsure whether you sampled the correct distribution, you can    #\n",
    "  #       sample a large batch and compute the empirical mean and variance using the #\n",
    "  #       .mean() and .var() functions.                                              #\n",
    "  # HINT: You can directly use model.decoder() to decode the samples.                #\n",
    "  ####################################################################################\n",
    "\n",
    "  #################################### END TODO ######################################\n",
    "\n",
    "  fig = plt.figure(figsize = (10, 10))   \n",
    "  ax1 = plt.subplot(111)\n",
    "  ax1.imshow(torchvision.utils.make_grid(decoded_samples[:16], nrow=4, pad_value=1.)\\\n",
    "                .data.cpu().numpy().transpose(1, 2, 0), cmap='gray')\n",
    "  plt.show()\n",
    "\n",
    "vis_samples(ae_model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ExC5BXn3rbap"
   },
   "source": [
    ">**Prob1-3 continued: Inline Question: Describe your observations, why do you think they occur? [2pt]** \n",
    ">(max 150 words)\n",
    ">\n",
    ">**Answer:** \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oSUezleArhCI"
   },
   "source": [
    "# 3. Variational Auto-Encoder (VAE)\n",
    "\n",
    "Variational auto-encoders use a very similar architecture to deterministic auto-encoders, but are inherently storchastic models, i.e. we perform a stochastic sampling operation during the forward pass, leading to different different outputs every time we run the network for the same input. This sampling is required to optimize the VAE objective also known as the evidence lower bound (ELBO):\n",
    "\n",
    "$$\n",
    "p(x) > \\underbrace{\\mathbb{E}_{z\\sim q(z\\vert x)} p(x \\vert z)}_{\\text{reconstruction}} - \\underbrace{D_{\\text{KL}}\\big(q(z \\vert x), p(z)\\big)}_{\\text{prior divergence}}\n",
    "$$\n",
    "\n",
    "Here, $D_{\\text{KL}}(q, p)$ denotes the Kullback-Leibler (KL) divergence between the posterior distribution $q(z \\vert x)$, i.e. the output of our encoder, and $p(z)$, the prior over the embedding variable $z$, which we can choose freely.\n",
    "\n",
    "For simplicity, we will choose a unit Gaussian prior again. The first term is the reconstruction term we already know from training the auto-encoder. When assuming a Gaussian output distribution for both encoder $q(z \\vert x)$ and decoder $p(x \\vert z)$ the objective reduces to:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{VAE}} = \\sum_{x\\sim \\mathcal{D}} \\mathcal{L}_{\\text{rec}}(x, \\hat{x}) - \\beta \\cdot D_{\\text{KL}}\\big(\\mathcal{N}(\\mu_q, \\sigma_q), \\mathcal{N}(0, I)\\big)\n",
    "$$\n",
    "\n",
    "Here, $\\hat{x}$ is the reconstruction output of the decoder. In comparison to the auto-encoder objective, the VAE adds a regularizing term between the output of the encoder and a chosen prior distribution, effectively forcing the encoder output to not stray too far from the prior during training. As a result the decoder gets trained with samples that look pretty similar to samples from the prior, which will hopefully allow us to generate better images when using the VAE as a generative model and actually feeding it samples from the prior (as we have done for the AE before).\n",
    "\n",
    "The coefficient $\\beta$ is a scalar weighting factor that trades off between reconstruction and regularization objective. We will investigate the influence of this factor in out experiments below.\n",
    "\n",
    "If you need a refresher on VAEs you can check out this tutorial paper: https://arxiv.org/abs/1606.05908\n",
    "\n",
    "### Reparametrization Trick\n",
    "\n",
    "The sampling procedure inside the VAE's forward pass for obtaining a sample $z$ from the posterior distribution $q(z \\vert x)$, when implemented naively, is non-differentiable. However, since $q(z\\vert x)$ is parametrized with a Gaussian function, there is a simple trick to obtain a differentiable sampling operator, known as the _reparametrization trick_.\n",
    "\n",
    "Instead of directly sampling $z \\sim \\mathcal{N}(\\mu_q, \\sigma_q)$ we can \"separate\" the network's predictions and the random sampling by computing the sample as:\n",
    "\n",
    "$$\n",
    "z = \\mu_q + \\sigma_q * \\epsilon , \\quad \\epsilon \\sim \\mathcal{N}(0, I)\n",
    "$$\n",
    "\n",
    "Note that in this equation, the sample $z$ is computed as a deterministic function of the network's predictions $\\mu_q$ and $\\sigma_q$ and therefore allows to propagate gradients through the sampling procedure.\n",
    "\n",
    "**Note**: While in the equations above the encoder network parametrizes the standard deviation $\\sigma_q$ of the Gaussian posterior distribution, in practice we usually parametrize the **logarithm of the standard deviation** $\\log \\sigma_q$ for numerical stability. Before sampling $z$ we will then exponentiate the network's output to obtain $\\sigma_q$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KSC7_Yy-n6-G"
   },
   "source": [
    "## Defining the VAE Model [7pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qofk8oQ9tAxd"
   },
   "outputs": [],
   "source": [
    "def kl_divergence(mu1, log_sigma1, mu2, log_sigma2):\n",
    "  \"\"\"Computes KL[p||q] between two Gaussians defined by [mu, log_sigma].\"\"\"\n",
    "  return (log_sigma2 - log_sigma1) + (torch.exp(log_sigma1) ** 2 + (mu1 - mu2) ** 2) \\\n",
    "               / (2 * torch.exp(log_sigma2) ** 2) - 0.5\n",
    "\n",
    "# Prob1-4\n",
    "class VAE(nn.Module):\n",
    "  def __init__(self, nz, beta=1.0):\n",
    "    super().__init__()\n",
    "    self.beta = beta          # factor trading off between two loss components\n",
    "    ####################################### TODO #######################################\n",
    "    # Instantiate Encoder and Decoder.                                                 #\n",
    "    # HINT: Remember that the encoder is now parametrizing a Gaussian distribution's   #\n",
    "    #       mean and log_sigma, so the dimensionality of the output needs to           #\n",
    "    #       double. The decoder works with an embedding sampled from this output.  #\n",
    "    ####################################################################################\n",
    "    \n",
    "    #################################### END TODO ######################################\n",
    "\n",
    "  def forward(self, x):\n",
    "    ####################################### TODO #######################################\n",
    "    # Implement the forward pass of the VAE.                                           #\n",
    "    # HINT: Your code should implement the following steps:                            #\n",
    "    #          1. encode input x, split encoding into mean and log_sigma of Gaussian   #\n",
    "    #          2. sample z from inferred posterior distribution using                  #\n",
    "    #             reparametrization trick                                              #\n",
    "    #          3. decode the sampled z to obtain the reconstructed image               #\n",
    "    ####################################################################################\n",
    "\n",
    "    #################################### END TODO ######################################\n",
    "\n",
    "    return {'q': q, \n",
    "            'rec': reconstruction}\n",
    "\n",
    "  def loss(self, x, outputs):\n",
    "    ####################################### TODO #######################################\n",
    "    # Implement the loss computation of the VAE.                                       #\n",
    "    # HINT: Your code should implement the following steps:                            #\n",
    "    #          1. compute the image reconstruction loss, similar to AE loss above      #\n",
    "    #          2. compute the KL divergence loss between the inferred posterior        #\n",
    "    #             distribution and a unit Gaussian prior; you can use the provided     #\n",
    "    #             function above for computing the KL divergence between two Gaussians #\n",
    "    #             parametrized by mean and log_sigma                                   #\n",
    "    # HINT: Make sure to compute the KL divergence in the correct order since it is    #\n",
    "    #       not symmetric!!  ie. KL(p, q) != KL(q, p)                                  #\n",
    "    ####################################################################################\n",
    "\n",
    "    #################################### END TODO ######################################\n",
    "\n",
    "    # return weighted objective\n",
    "    return rec_loss + self.beta * kl_loss, \\\n",
    "           {'rec_loss': rec_loss, 'kl_loss': kl_loss}\n",
    "    \n",
    "  def reconstruct(self, x):\n",
    "    \"\"\"Use mean of posterior estimate for visualization reconstruction.\"\"\"\n",
    "    ####################################### TODO #######################################\n",
    "    # This function is used for visualizing reconstructions of our VAE model. To       #\n",
    "    # obtain the maximum likelihood estimate we bypass the sampling procedure of the   #\n",
    "    # inferred latent and instead directly use the mean of the inferred posterior.     #\n",
    "    # HINT: encode the input image and then decode the mean of the posterior to obtain #\n",
    "    #       the reconstruction.                                                        #\n",
    "    ####################################################################################\n",
    "\n",
    "\n",
    "    #################################### END TODO ######################################\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EPCQZr-s_INx"
   },
   "source": [
    "## Setting up the VAE Training Loop [4pt]\n",
    "\n",
    "Let's start training the VAE model! We will first verify our implementation by setting $\\beta = 0$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2StBTjj__HBU"
   },
   "outputs": [],
   "source": [
    "# Prob1-5 VAE training loop\n",
    "learning_rate = 1e-3\n",
    "nz = 32\n",
    "beta = 0\n",
    "\n",
    "####################################### TODO #######################################\n",
    "epochs = 5      # recommended 5-20 epochs\n",
    "#################################### END TODO ######################################\n",
    "\n",
    "# build VAE model\n",
    "vae_model = VAE(nz, beta).to(device)    # transfer model to GPU if available\n",
    "vae_model = vae_model.train()   # set model in train mode (eg batchnorm params get updated)\n",
    "\n",
    "# build optimizer and loss function\n",
    "####################################### TODO #######################################\n",
    "# Build the optimizer for the vae_model. We will again use the Adam optimizer with #\n",
    "# the given learning rate and otherwise default parameters.                        #\n",
    "####################################################################################\n",
    "# same as AE\n",
    "#################################### END TODO ######################################\n",
    "\n",
    "train_it = 0\n",
    "rec_loss, kl_loss = [], []\n",
    "print(f\"Running {epochs} epochs with {beta=}\")\n",
    "for ep in range(epochs):\n",
    "  print(\"Run Epoch {}\".format(ep))\n",
    "  ####################################### TODO #######################################\n",
    "  # Implement the main training loop for the VAE model.                              #\n",
    "  # HINT: Your training loop should sample batches from the data loader, run the     #\n",
    "  #       forward pass of the VAE, compute the loss, perform the backward pass and   #\n",
    "  #       perform one gradient step with the optimizer.                              #\n",
    "  # HINT: Don't forget to erase old gradients before performing the backward pass.   #\n",
    "  # HINT: This time we will use the loss() function of our model for computing the   #\n",
    "  #       training loss. It outputs the total training loss and a dict containing    #\n",
    "  #       the breakdown of reconstruction and KL loss.                               #\n",
    "  ####################################################################################\n",
    "  for ...:\n",
    "\n",
    "\n",
    "    rec_loss.append(losses['rec_loss']); kl_loss.append(losses['kl_loss'])\n",
    "    if train_it % 100 == 0:\n",
    "      print(\"It {}: Total Loss: {}, \\t Rec Loss: {},\\t KL Loss: {}\"\\\n",
    "            .format(train_it, total_loss, losses['rec_loss'], losses['kl_loss']))\n",
    "    train_it += 1\n",
    "  #################################### END TODO ####################################\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "rec_loss_plotdata = [foo.detach().cpu() for foo in rec_loss]\n",
    "kl_loss_plotdata = [foo.detach().cpu() for foo in kl_loss]\n",
    "\n",
    "# log the loss training curves\n",
    "fig = plt.figure(figsize = (10, 5))   \n",
    "ax1 = plt.subplot(121)\n",
    "ax1.plot(rec_loss_plotdata)\n",
    "ax1.title.set_text(\"Reconstruction Loss\")\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.plot(kl_loss_plotdata)\n",
    "ax2.title.set_text(\"KL Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YDK4A6WQPgUi"
   },
   "source": [
    "Let's look at some reconstructions and decoded embedding samples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2KnFxjABPmQP"
   },
   "outputs": [],
   "source": [
    "# visualize VAE reconstructions and samples from the generative model\n",
    "vis_reconstruction(vae_model, randomize=True)\n",
    "vis_samples(vae_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BqFtLvsIEwRy"
   },
   "source": [
    "## Tweaking the loss function $\\beta$ [2pt]\n",
    "Prob1-6: Let's repeat the same experiment for $\\beta = 10$, a very high value for the coefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NvQ23uV5PjXL"
   },
   "outputs": [],
   "source": [
    "# VAE training loop\n",
    "learning_rate = 1e-3\n",
    "nz = 32\n",
    "beta = 10\n",
    "\n",
    "####################################### TODO #######################################\n",
    "epochs = 5      # recommended 5-20 epochs\n",
    "#################################### END TODO ######################################\n",
    "\n",
    "# build VAE model\n",
    "vae_model = VAE(nz, beta).to(device)    # transfer model to GPU if available\n",
    "vae_model = vae_model.train()   # set model in train mode (eg batchnorm params get updated)\n",
    "\n",
    "# build optimizer and loss function\n",
    "####################################### TODO #######################################\n",
    "# Build the optimizer for the vae_model. We will again use the Adam optimizer with #\n",
    "# the given learning rate and otherwise default parameters.                        #\n",
    "####################################################################################\n",
    "# same as AE\n",
    "#################################### END TODO ######################################\n",
    "\n",
    "train_it = 0\n",
    "rec_loss, kl_loss = [], []\n",
    "print(f\"Running {epochs} epochs with {beta=}\")\n",
    "for ep in range(epochs):\n",
    "  print(\"Run Epoch {}\".format(ep))\n",
    "  ####################################### TODO #######################################\n",
    "  # Implement the main training loop for the VAE model.                              #\n",
    "  # HINT: Your training loop should sample batches from the data loader, run the     #\n",
    "  #       forward pass of the VAE, compute the loss, perform the backward pass and   #\n",
    "  #       perform one gradient step with the optimizer.                              #\n",
    "  # HINT: Don't forget to erase old gradients before performing the backward pass.   #\n",
    "  # HINT: This time we will use the loss() function of our model for computing the   #\n",
    "  #       training loss. It outputs the total training loss and a dict containing    #\n",
    "  #       the breakdown of reconstruction and KL loss.                               #\n",
    "  ####################################################################################\n",
    "  for ...:\n",
    "\n",
    "\n",
    "    rec_loss.append(losses['rec_loss']); kl_loss.append(losses['kl_loss'])\n",
    "    if train_it % 100 == 0:\n",
    "      print(\"It {}: Total Loss: {}, \\t Rec Loss: {},\\t KL Loss: {}\"\\\n",
    "            .format(train_it, total_loss, losses['rec_loss'], losses['kl_loss']))\n",
    "    train_it += 1\n",
    "  #################################### END TODO ####################################\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "rec_loss_plotdata = [foo.detach().cpu() for foo in rec_loss]\n",
    "kl_loss_plotdata = [foo.detach().cpu() for foo in kl_loss]\n",
    "\n",
    "# log the loss training curves\n",
    "fig = plt.figure(figsize = (10, 5))   \n",
    "ax1 = plt.subplot(121)\n",
    "ax1.plot(rec_loss_plotdata)\n",
    "ax1.title.set_text(\"Reconstruction Loss\")\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.plot(kl_loss_plotdata)\n",
    "ax2.title.set_text(\"KL Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M_pdzfj--tUB"
   },
   "source": [
    ">**Inline Question: What can you observe when setting $\\beta = 0$ and $\\beta = 10$? Explain your observations! [2pt]** \n",
    ">(max 200 words) \n",
    ">\n",
    ">**Answer**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MpznmVGvGXjO"
   },
   "source": [
    "## Obtaining the best $\\beta$-factor [5pt]\n",
    "Prob 1-6 continued: Now we can start tuning the beta value to achieve a good result. First describe what a \"good result\" would look like (focus what you would expect for reconstructions and sample quality). \n",
    "\n",
    ">**Inline Question: Characterize what properties you would expect for reconstructions and samples of a well-tuned VAE! [3pt]**\n",
    ">(max 200 words)\n",
    ">\n",
    ">**Answer**: \n",
    ">\n",
    "\n",
    "Now that you know what outcome we would like to obtain, try to tune $\\beta$ to achieve this result. Logarithmic search in steps of 10x will be helpful, good results can be achieved after ~20 epochs of training. Training reconstructions should be high quality, test samples should be diverse, distinguishable numbers, most samples recognizable as numbers.\n",
    "\n",
    "**Answer: Tuned beta value _______ [2pt]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5d3Q5PD3QZs8"
   },
   "outputs": [],
   "source": [
    "# Tuning for best beta\n",
    "learning_rate = 1e-3\n",
    "nz = 32\n",
    "\n",
    "####################################### TODO #######################################\n",
    "epochs = 5      # recommended 5-20 epochs\n",
    "beta = ... # Tune this for best results\n",
    "#################################### END TODO ######################################\n",
    "\n",
    "# build VAE model\n",
    "vae_model = VAE(nz, beta).to(device)    # transfer model to GPU if available\n",
    "vae_model = vae_model.train()   # set model in train mode (eg batchnorm params get updated)\n",
    "\n",
    "# build optimizer and loss function\n",
    "####################################### TODO #######################################\n",
    "# Build the optimizer for the vae_model. We will again use the Adam optimizer with #\n",
    "# the given learning rate and otherwise default parameters.                        #\n",
    "####################################################################################\n",
    "# same as AE\n",
    "#################################### END TODO ######################################\n",
    "\n",
    "train_it = 0\n",
    "rec_loss, kl_loss = [], []\n",
    "print(f\"Running {epochs} epochs with {beta=}\")\n",
    "for ep in range(epochs):\n",
    "  print(\"Run Epoch {}\".format(ep))\n",
    "  ####################################### TODO #######################################\n",
    "  # Implement the main training loop for the VAE model.                              #\n",
    "  # HINT: Your training loop should sample batches from the data loader, run the     #\n",
    "  #       forward pass of the VAE, compute the loss, perform the backward pass and   #\n",
    "  #       perform one gradient step with the optimizer.                              #\n",
    "  # HINT: Don't forget to erase old gradients before performing the backward pass.   #\n",
    "  # HINT: This time we will use the loss() function of our model for computing the   #\n",
    "  #       training loss. It outputs the total training loss and a dict containing    #\n",
    "  #       the breakdown of reconstruction and KL loss.                               #\n",
    "  ####################################################################################\n",
    "  for ...:\n",
    "\n",
    "\n",
    "    rec_loss.append(losses['rec_loss']); kl_loss.append(losses['kl_loss'])\n",
    "    if train_it % 100 == 0:\n",
    "      print(\"It {}: Total Loss: {}, \\t Rec Loss: {},\\t KL Loss: {}\"\\\n",
    "            .format(train_it, total_loss, losses['rec_loss'], losses['kl_loss']))\n",
    "    train_it += 1\n",
    "  #################################### END TODO ####################################\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "rec_loss_plotdata = [foo.detach().cpu() for foo in rec_loss]\n",
    "kl_loss_plotdata = [foo.detach().cpu() for foo in kl_loss]\n",
    "\n",
    "# log the loss training curves\n",
    "fig = plt.figure(figsize = (10, 5))   \n",
    "ax1 = plt.subplot(121)\n",
    "ax1.plot(rec_loss_plotdata)\n",
    "ax1.title.set_text(\"Reconstruction Loss\")\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.plot(kl_loss_plotdata)\n",
    "ax2.title.set_text(\"KL Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VtU4-fHnAbYL"
   },
   "source": [
    "# 4. Embedding Space Interpolation [3pt]\n",
    "\n",
    "As mentioned in the introduction, AEs and VAEs cannot only be used to generate images, but also to learn low-dimensional representations of their inputs. In this final section we will investigate the representations we learned with both models by **interpolating in embedding space** between different images. We will encode two images into their low-dimensional embedding representations, then interpolate these embeddings and reconstruct the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9VO_Dj-nAw6z"
   },
   "outputs": [],
   "source": [
    "# Prob1-7\n",
    "nz=32\n",
    "\n",
    "def get_image_with_label(target_label):\n",
    "  \"\"\"Returns a random image from the training set with the requested digit.\"\"\"\n",
    "  for img_batch, label_batch in mnist_data_loader:\n",
    "    for img, label in zip(img_batch, label_batch):\n",
    "      if label == target_label:\n",
    "        return img.to(device)\n",
    "\n",
    "def interpolate_and_visualize(model, tag, start_img, end_img):\n",
    "  \"\"\"Encodes images and performs interpolation. Displays decodings.\"\"\"\n",
    "  model.eval()    # put model in eval mode to avoid updating batchnorm\n",
    "\n",
    "  # encode both images into embeddings (use posterior mean for interpolation)\n",
    "  z_start = model.encoder(start_img[None].reshape(1,784))[..., :nz]\n",
    "  z_end = model.encoder(end_img[None].reshape(1,784))[..., :nz]\n",
    "\n",
    "  # compute interpolated latents\n",
    "  N_INTER_STEPS = 5\n",
    "  z_inter = [z_start + i/N_INTER_STEPS * (z_end - z_start) for i in range(N_INTER_STEPS)]\n",
    "\n",
    "  # decode interpolated embeddings (as a single batch)\n",
    "  img_inter = model.decoder(torch.cat(z_inter))\n",
    "  img_inter = img_inter.reshape(-1, 28, 28)\n",
    "\n",
    "  # reshape result and display interpolation\n",
    "  vis_imgs = torch.cat([start_img, img_inter, end_img]).reshape(-1,1,28,28)\n",
    "  fig = plt.figure(figsize = (10, 10))   \n",
    "  ax1 = plt.subplot(111)\n",
    "  ax1.imshow(torchvision.utils.make_grid(vis_imgs, nrow=N_INTER_STEPS+2, pad_value=1.)\\\n",
    "                  .data.cpu().numpy().transpose(1, 2, 0), cmap='gray')\n",
    "  plt.title(tag)\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "### Interpolation 1\n",
    "START_LABEL = # ... TODO CHOOSE\n",
    "END_LABEL = # ... TODO CHOOSE\n",
    "# sample two training images with given labels\n",
    "start_img = get_image_with_label(START_LABEL)\n",
    "end_img = get_image_with_label(END_LABEL)\n",
    "# visualize interpolations for AE and VAE models\n",
    "interpolate_and_visualize(ae_model, \"Auto-Encoder\", start_img, end_img)\n",
    "interpolate_and_visualize(vae_model, \"Variational Auto-Encoder\", start_img, end_img)\n",
    "\n",
    "### Interpolation 2\n",
    "START_LABEL = # ... TODO CHOOSE\n",
    "END_LABEL = # ... TODO CHOOSE\n",
    "# sample two training images with given labels\n",
    "start_img = get_image_with_label(START_LABEL)\n",
    "end_img = get_image_with_label(END_LABEL)\n",
    "# visualize interpolations for AE and VAE models\n",
    "interpolate_and_visualize(ae_model, \"Auto-Encoder\", start_img, end_img)\n",
    "interpolate_and_visualize(vae_model, \"Variational Auto-Encoder\", start_img, end_img)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdQKHspOF_5Q"
   },
   "source": [
    "Repeat the experiment for different start / end labels and different samples. Describe your observations.\n",
    "\n",
    ">**Prob1-7 continued: Inline Question: Repeat the interpolation experiment with different start / end labels and multiple samples. Describe your observations! [2 pt]**\n",
    ">  1. How do AE and VAE embedding space interpolations differ?\n",
    ">  2. How do you expect these differences to affect the usefulness of the learned representation for downstream learning?\n",
    ">(max 300 words)\n",
    ">\n",
    ">**Answer**:\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4AuOGRUXwwG"
   },
   "source": [
    "# 5. Conditional VAE\n",
    "Let us now try a Conditional VAE\n",
    "Now we will try to create a [Conditional VAE](https://proceedings.neurips.cc/paper/2014/file/d523773c6b194f37b938d340d5d02232-Paper.pdf), where we can condition the encoder and decoder of the VAE on the label `c`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DkAuIglQXwwG"
   },
   "source": [
    "## Defining the conditional Encoder, Decoder, and VAE models [5 pt]\n",
    "\n",
    "Prob1-8. We create a separate encoder and decoder class that take in an additional argument `c` in their forward pass, and then build our CVAE model on top of it. Note that the encoder and decoder just need to append `c` to the standard inputs to these modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "33EJfCfBXwwG"
   },
   "outputs": [],
   "source": [
    "def idx2onehot(idx, n):\n",
    "    \"\"\"Converts a batch of indices to a one-hot representation.\"\"\"\n",
    "    assert torch.max(idx).item() < n\n",
    "    if idx.dim() == 1:\n",
    "        idx = idx.unsqueeze(1)\n",
    "    onehot = torch.zeros(idx.size(0), n).to(idx.device)\n",
    "    onehot.scatter_(1, idx, 1)\n",
    "\n",
    "    return onehot\n",
    "\n",
    "# Let's define encoder and decoder networks\n",
    "\n",
    "class CVAEEncoder(nn.Module):\n",
    "  def __init__(self, nz, input_size, conditional, num_labels):\n",
    "    super().__init__()\n",
    "    self.input_size = input_size + num_labels if conditional else input_size\n",
    "    self.num_labels = num_labels\n",
    "    self.conditional = conditional\n",
    "\n",
    "    ################################# TODO #########################################\n",
    "    # Create the network architecture using a nn.Sequential module wrapper.        #\n",
    "    # Encoder Architecture:                                                        #\n",
    "    # - input_size -> 256                                                          #\n",
    "    # - ReLU                                                                       #\n",
    "    # - 256 -> 64                                                                  #\n",
    "    # - ReLU                                                                       #\n",
    "    # - 64 -> nz                                                                   #\n",
    "    # HINT: Verify the shapes of intermediate layers by running partial networks   #\n",
    "    #        (with the next notebook cell) and visualizing the output shapes.      #\n",
    "    ################################################################################\n",
    "\n",
    "    ################################ END TODO ######################################\n",
    "\n",
    "  def forward(self, x, c=None):\n",
    "    ################################# TODO #########################################\n",
    "    # If using conditional VAE, concatenate x and a onehot version of c to create  #\n",
    "    # the full input. Use function idx2onehot above.                               #\n",
    "    ################################################################################\n",
    "\n",
    "    ################################################################################\n",
    "    return self.net(x)\n",
    "\n",
    "\n",
    "class CVAEDecoder(nn.Module):\n",
    "  def __init__(self, nz, output_size, conditional, num_labels):\n",
    "    super().__init__()\n",
    "    self.output_size = output_size\n",
    "    self.conditional = conditional\n",
    "    self.num_labels = num_labels\n",
    "    if self.conditional:\n",
    "        nz = nz + num_labels\n",
    "    ################################# TODO #########################################\n",
    "    # Create the network architecture using a nn.Sequential module wrapper.        #\n",
    "    # Decoder Architecture (mirrors encoder architecture):                         #\n",
    "    # - nz -> 64                                                                   #\n",
    "    # - ReLU                                                                       #\n",
    "    # - 64 -> 256                                                                  #\n",
    "    # - ReLU                                                                       #\n",
    "    # - 256 -> output_size                                                         #\n",
    "    ################################################################################\n",
    "\n",
    "    ################################ END TODO #######################################\n",
    "\n",
    "  def forward(self, z, c=None):\n",
    "    ################################# TODO #########################################\n",
    "    # If using conditional VAE, concatenate z and a onehot version of c to create  #\n",
    "    # the full embedding. Use function idx2onehot above.                           #\n",
    "    ################################################################################\n",
    "\n",
    "    ################################ END TODO #######################################\n",
    "\n",
    "    return self.net(z).reshape(-1, 1, self.output_size)\n",
    "\n",
    "\n",
    "class CVAE(nn.Module):\n",
    "    def __init__(self, nz, beta=1.0, conditional=False, num_labels=0):\n",
    "        super().__init__()\n",
    "        if conditional:\n",
    "            assert num_labels > 0\n",
    "        self.beta = beta\n",
    "        self.encoder = CVAEEncoder(2*nz, input_size=in_size, conditional=conditional, num_labels=num_labels)\n",
    "        self.decoder = CVAEDecoder(nz, output_size=out_size, conditional=conditional, num_labels=num_labels)\n",
    "\n",
    "    def forward(self, x, c=None):\n",
    "        if x.dim() > 2:\n",
    "            x = x.view(-1, 28*28)\n",
    "\n",
    "        q = self.encoder(x,c)\n",
    "        mu, log_sigma = torch.chunk(q, 2, dim=-1)\n",
    "\n",
    "        # sample latent variable z with reparametrization\n",
    "        eps = torch.normal(mean=torch.zeros_like(mu), std=torch.ones_like(log_sigma))\n",
    "        # eps = torch.randn_like(mu) # Alternatively use this\n",
    "        z = mu + eps * torch.exp(log_sigma)\n",
    "\n",
    "        # compute reconstruction\n",
    "        reconstruction = self.decoder(z, c)\n",
    "\n",
    "        return {'q': q, 'rec': reconstruction, 'c': c}\n",
    "\n",
    "    def loss(self, x, outputs):\n",
    "        ####################################### TODO #######################################\n",
    "        # Implement the loss computation of the VAE.                                       #\n",
    "        # HINT: Your code should implement the following steps:                            #\n",
    "        #          1. compute the image reconstruction loss, similar to AE loss above      #\n",
    "        #          2. compute the KL divergence loss between the inferred posterior        #\n",
    "        #             distribution and a unit Gaussian prior; you can use the provided     #\n",
    "        #             function above for computing the KL divergence between two Gaussians #\n",
    "        #             parametrized by mean and log_sigma                                   #\n",
    "        # HINT: Make sure to compute the KL divergence in the correct order since it is    #\n",
    "        #       not symmetric!!  ie. KL(p, q) != KL(q, p)                                  #\n",
    "        ####################################################################################\n",
    "\n",
    "        #################################### END TODO ######################################\n",
    "\n",
    "        # return weighted objective\n",
    "        return rec_loss + self.beta * kl_loss, \\\n",
    "            {'rec_loss': rec_loss, 'kl_loss': kl_loss}\n",
    "    \n",
    "    def reconstruct(self, x, c=None):\n",
    "        \"\"\"Use mean of posterior estimate for visualization reconstruction.\"\"\"\n",
    "        ####################################### TODO #######################################\n",
    "        # This function is used for visualizing reconstructions of our VAE model. To       #\n",
    "        # obtain the maximum likelihood estimate we bypass the sampling procedure of the   #\n",
    "        # inferred latent and instead directly use the mean of the inferred posterior.     #\n",
    "        # HINT: encode the input image and then decode the mean of the posterior to obtain #\n",
    "        #       the reconstruction.                                                        #\n",
    "        ####################################################################################\n",
    "\n",
    "        #################################### END TODO ######################################\n",
    "        return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAPxvyUsXwwH"
   },
   "source": [
    "## Setting up the CVAE Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fwpfvKaqXwwH"
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "nz = 32\n",
    "\n",
    "####################################### TODO #######################################\n",
    "# Tune the beta parameter to obtain good training results. However, for the    #\n",
    "# initial experiments leave beta = 0 in order to verify our implementation.        #\n",
    "####################################################################################\n",
    "epochs = 5 # works with fewer epochs than AE, VAE. we only test conditional samples.\n",
    "beta = 0\n",
    "#################################### END TODO ######################################\n",
    "\n",
    "# build CVAE model\n",
    "conditional = True\n",
    "cvae_model = CVAE(nz, beta, conditional=conditional, num_labels=10).to(device)    # transfer model to GPU if available\n",
    "cvae_model = cvae_model.train()   # set model in train mode (eg batchnorm params get updated)\n",
    "\n",
    "# build optimizer and loss function\n",
    "####################################### TODO #######################################\n",
    "# Build the optimizer for the cvae_model. We will again use the Adam optimizer with #\n",
    "# the given learning rate and otherwise default parameters.                        #\n",
    "####################################################################################\n",
    "# same as AE\n",
    "#################################### END TODO ######################################\n",
    "\n",
    "train_it = 0\n",
    "rec_loss, kl_loss = [], []\n",
    "print(f\"Running {epochs} epochs with {beta=}\")\n",
    "for ep in range(epochs):\n",
    "  print(f\"Run Epoch {ep}\")\n",
    "  ####################################### TODO #######################################\n",
    "  # Implement the main training loop for the model.                                  #\n",
    "  # If using conditional VAE, remember to pass the conditional variable c to the     #\n",
    "  # forward pass                                                                     #\n",
    "  # HINT: Your training loop should sample batches from the data loader, run the     #\n",
    "  #       forward pass of the model, compute the loss, perform the backward pass and #\n",
    "  #       perform one gradient step with the optimizer.                              #\n",
    "  # HINT: Don't forget to erase old gradients before performing the backward pass.   #\n",
    "  # HINT: As before, we will use the loss() function of our model for computing the  #\n",
    "  #       training loss. It outputs the total training loss and a dict containing    #\n",
    "  #       the breakdown of reconstruction and KL loss.                               #\n",
    "  ####################################################################################\n",
    "  for ...:\n",
    "\n",
    "    rec_loss.append(losses['rec_loss']); kl_loss.append(losses['kl_loss'])\n",
    "    if train_it % 100 == 0:\n",
    "      print(\"It {}: Total Loss: {}, \\t Rec Loss: {},\\t KL Loss: {}\"\\\n",
    "            .format(train_it, total_loss, losses['rec_loss'], losses['kl_loss']))\n",
    "    train_it += 1\n",
    "  #################################### END TODO ####################################\n",
    "\n",
    "print(\"Done!\")\n",
    "\n",
    "rec_loss_plotdata = [foo.detach().cpu() for foo in rec_loss]\n",
    "kl_loss_plotdata = [foo.detach().cpu() for foo in kl_loss]\n",
    "\n",
    "# log the loss training curves\n",
    "fig = plt.figure(figsize = (10, 5))\n",
    "ax1 = plt.subplot(121)\n",
    "ax1.plot(rec_loss_plotdata)\n",
    "ax1.title.set_text(\"Reconstruction Loss\")\n",
    "ax2 = plt.subplot(122)\n",
    "ax2.plot(kl_loss_plotdata)\n",
    "ax2.title.set_text(\"KL Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePn5sFKCXwwH"
   },
   "source": [
    "### Verifying conditional samples from CVAE [6 pt]\n",
    "Now let us generate samples from the trained model, conditioned on all the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AQOCmLCXwwI"
   },
   "outputs": [],
   "source": [
    "# Prob1-9\n",
    "if conditional:\n",
    "    c = torch.arange(0, 10).long().unsqueeze(1).to(device)\n",
    "    z = torch.randn([10, nz]).to(device)\n",
    "    x = cvae_model.decoder(z, c=c)\n",
    "else:\n",
    "    z = torch.randn([10, nz]).to(device)\n",
    "    x = cvae_model.decoder(z)\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize=(5, 10))\n",
    "for p in range(10):\n",
    "    plt.subplot(5, 2, p+1)\n",
    "    if conditional:\n",
    "        plt.text(\n",
    "            0, 0, \"c={:d}\".format(c[p].item()), color='black',\n",
    "            backgroundcolor='white', fontsize=8)\n",
    "    plt.imshow(x[p].view(28, 28).cpu().data.numpy(), cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytPPsGz2vBx0"
   },
   "source": [
    "# Submission Instructions\n",
    "\n",
    "You need to submit this jupyter notebook and a PDF. See Piazza for detailed submission instructions."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "30a36b704feb31e8d62934e0f6235a63429541b16e7656344ff7eed2d46d043a"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "00ccb5258094474484c970e4b698d0e9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "074608e2e6c646d8be29bee287d4eecd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "08d643e9e1ed4f5f8a4b938513eb74b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f8f09be5e62e446cb81ea9ba618a2b05",
      "placeholder": "​",
      "style": "IPY_MODEL_cc44f8e2f9d94dac82132168dfa615cb",
      "value": " 28881/28881 [00:00&lt;00:00, 807119.31it/s]"
     }
    },
    "0c837a309a8749e9acb998d26b53eb03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1069f776c52b4b53be617bbf368fcc5e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "16c9106fb10d43c28815c73e0a960c32": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5dbc78cf70d64fc094c758bc5c06b8bb",
      "placeholder": "​",
      "style": "IPY_MODEL_d80c21ce156748ddae0ff0bcccaccf41",
      "value": "100%"
     }
    },
    "251a7d7cccc243a7a9afb379f40b6abc": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25c51dbd460048a0ade87d231b919eba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_2cb89c5dca2444aa899dc73dc3e281f5",
       "IPY_MODEL_e117f794f56b4c45894647d748a0f6f4",
       "IPY_MODEL_61ede69ade0243fb8b9fbcdf8ccabf61"
      ],
      "layout": "IPY_MODEL_ec99aa403334405184322466cdef6ded"
     }
    },
    "2cb89c5dca2444aa899dc73dc3e281f5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_074608e2e6c646d8be29bee287d4eecd",
      "placeholder": "​",
      "style": "IPY_MODEL_e41aa50f2c594af084fa1af6b9d9a4b3",
      "value": "100%"
     }
    },
    "316e858872234de9a3300a355a32df5d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "430e2502449c47a5a23f47eaf088a894": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4c207577fbf14d098de29220c1a200d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d09b2b8b271b46788d00e6a1442699de",
       "IPY_MODEL_f321c1db62ad4645916b08460fd6b40a",
       "IPY_MODEL_08d643e9e1ed4f5f8a4b938513eb74b2"
      ],
      "layout": "IPY_MODEL_1069f776c52b4b53be617bbf368fcc5e"
     }
    },
    "53b496dc0326451994983d34e32eabcf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c84c00ce102843c1a5567338fa433747",
      "placeholder": "​",
      "style": "IPY_MODEL_c1e61faf117747e49ba18e9e8c07c359",
      "value": " 4542/4542 [00:00&lt;00:00, 84286.54it/s]"
     }
    },
    "55b2698f8bab4a2e9b40bad1ac28580c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5795e5119e744f3b95af1061b694019d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5d868cebd70a4bcf9377ffb62f9adb86": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_16c9106fb10d43c28815c73e0a960c32",
       "IPY_MODEL_d885bf767bc64103a16be48fce9cb9a9",
       "IPY_MODEL_7aab706d77ee449881a0a1e199a8e845"
      ],
      "layout": "IPY_MODEL_251a7d7cccc243a7a9afb379f40b6abc"
     }
    },
    "5dbc78cf70d64fc094c758bc5c06b8bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "61ede69ade0243fb8b9fbcdf8ccabf61": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d6abdefd833a4850a6d4fc5532a0a31f",
      "placeholder": "​",
      "style": "IPY_MODEL_ec1a9588948e46bb85f9a2cfc2afe690",
      "value": " 9912422/9912422 [00:00&lt;00:00, 166253633.90it/s]"
     }
    },
    "669a51ec535b4841b19bafc1cf734534": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "79a7c29698064a238d27747ed85b3442": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "7aab706d77ee449881a0a1e199a8e845": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_00ccb5258094474484c970e4b698d0e9",
      "placeholder": "​",
      "style": "IPY_MODEL_669a51ec535b4841b19bafc1cf734534",
      "value": " 1648877/1648877 [00:00&lt;00:00, 28279261.67it/s]"
     }
    },
    "a11343c41bcd4a71912a80830d5bd5b6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1e61faf117747e49ba18e9e8c07c359": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c84c00ce102843c1a5567338fa433747": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cc44f8e2f9d94dac82132168dfa615cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d09b2b8b271b46788d00e6a1442699de": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5795e5119e744f3b95af1061b694019d",
      "placeholder": "​",
      "style": "IPY_MODEL_e777a7f4889840be88dffb206d851bf4",
      "value": "100%"
     }
    },
    "d1396522b53047149210c329510307fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0c837a309a8749e9acb998d26b53eb03",
      "max": 4542,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_79a7c29698064a238d27747ed85b3442",
      "value": 4542
     }
    },
    "d4caee53c3e049c7ba72e00ba03d358a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_55b2698f8bab4a2e9b40bad1ac28580c",
      "placeholder": "​",
      "style": "IPY_MODEL_eee3ae88d3db4a05b23b7f6e14e176cb",
      "value": "100%"
     }
    },
    "d6abdefd833a4850a6d4fc5532a0a31f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d80c21ce156748ddae0ff0bcccaccf41": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d885bf767bc64103a16be48fce9cb9a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a11343c41bcd4a71912a80830d5bd5b6",
      "max": 1648877,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_f5ee2f461d0c41e8b03f2450ccb0845b",
      "value": 1648877
     }
    },
    "dd354536c91840b8af906237e2ad1540": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "dece586b88a74ffbb5dba17157cac40c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e117f794f56b4c45894647d748a0f6f4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_dd354536c91840b8af906237e2ad1540",
      "max": 9912422,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ebf2a93e8e29426c9a944ff9f6e53a2a",
      "value": 9912422
     }
    },
    "e41aa50f2c594af084fa1af6b9d9a4b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e777a7f4889840be88dffb206d851bf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ebf2a93e8e29426c9a944ff9f6e53a2a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ec1a9588948e46bb85f9a2cfc2afe690": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec99aa403334405184322466cdef6ded": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eee3ae88d3db4a05b23b7f6e14e176cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f321c1db62ad4645916b08460fd6b40a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_430e2502449c47a5a23f47eaf088a894",
      "max": 28881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_316e858872234de9a3300a355a32df5d",
      "value": 28881
     }
    },
    "f39a85e3cf3f4fdfb8141c16c90f5cee": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d4caee53c3e049c7ba72e00ba03d358a",
       "IPY_MODEL_d1396522b53047149210c329510307fa",
       "IPY_MODEL_53b496dc0326451994983d34e32eabcf"
      ],
      "layout": "IPY_MODEL_dece586b88a74ffbb5dba17157cac40c"
     }
    },
    "f5ee2f461d0c41e8b03f2450ccb0845b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f8f09be5e62e446cb81ea9ba618a2b05": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
